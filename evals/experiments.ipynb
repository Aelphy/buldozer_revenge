{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lasagne \n",
    "import theano\n",
    "\n",
    "import theano.tensor as T\n",
    "\n",
    "from lasagne.layers import InputLayer, Conv2DLayer, MaxPool2DLayer, DenseLayer, DropoutLayer, batch_norm, NonlinearityLayer\n",
    "from lasagne.nonlinearities import softmax, elu, rectify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_network(input_shape, input_var):\n",
    "    net = InputLayer(input_shape)\n",
    "    net = Conv2DLayer(net, num_filters=64, filter_size=5, nonlinearity=rectify)\n",
    "    net = MaxPool2DLayer(net, pool_size=2)\n",
    "    net = Conv2DLayer(net, num_filters=128, filter_size=3, nonlinearity=rectify)\n",
    "    net = MaxPool2DLayer(net, pool_size=2)\n",
    "    net = Conv2DLayer(net, num_filters=256, filter_size=3, nonlinearity=rectify)\n",
    "    net = DenseLayer(net, num_units=1024, nonlinearity=rectify)\n",
    "    net = DenseLayer(net, num_units=10, nonlinearity=softmax)\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_X = T.tensor4('input')\n",
    "target = T.ivector('target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nn = build_network((None, 3, 32, 32), input_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = lasagne.layers.get_all_params(nn, trainable=True)\n",
    "output = lasagne.layers.get_output(nn, input_X, deterministic=False)\n",
    "obj = lasagne.objectives.categorical_crossentropy(output, target).mean()\n",
    "updates = lasagne.updates.adamax(obj, params, learning_rate=1e-3)\n",
    "accuracy = lasagne.objectives.categorical_accuracy(output, target).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = theano.function([input_X, target], [obj, accuracy], updates=updates)\n",
    "evaluate = theano.function([input_X, target], accuracy)\n",
    "predict = theano.function([input_X], lasagne.layers.get_output(nn, input_X, deterministic=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    \n",
    "    if inputs.shape[0] < batchsize:\n",
    "        yield inputs, targets\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]\n",
    "        \n",
    "\n",
    "def learn(dataset, num_epochs=100, batchsize=500):\n",
    "    '''\n",
    "        num_epochs - amount of passes through the data\n",
    "        dataset - [X_train, X_val, y_train, y_val]\n",
    "    '''\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = dataset\n",
    "    objectives = [None] * num_epochs\n",
    "    best_model = None\n",
    "    best_acc = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # In each epoch, we do a full pass over the training data:\n",
    "        train_err = 0\n",
    "        train_acc = 0\n",
    "        train_batches = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        for batch in iterate_minibatches(X_train, y_train, batchsize, True):\n",
    "            inputs, targets = batch\n",
    "            train_err_batch, train_acc_batch = train(inputs, targets)\n",
    "            train_err += train_err_batch\n",
    "            train_acc += train_acc_batch\n",
    "            train_batches += 1\n",
    "\n",
    "        # And a full pass over the validation data:\n",
    "        val_acc = 0\n",
    "        val_batches = 0\n",
    "\n",
    "        for batch in iterate_minibatches(X_val, y_val, batchsize):\n",
    "            inputs, targets = batch\n",
    "            val_acc += evaluate(inputs, targets)\n",
    "            val_batches += 1\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            # Then we print the results for this epoch:\n",
    "            print('Epoch {} of {} took {:.3f}s'.format(epoch, num_epochs, time.time() - start_time))\n",
    "            print('  training loss (in-iteration):\\t\\t{:.6f}'.format(train_err / train_batches))\n",
    "            print('  train accuracy:\\t\\t{:.2f} %'.format(train_acc / train_batches * 100))\n",
    "            print('  validation accuracy:\\t\\t{:.2f} %'.format(val_acc / val_batches * 100))\n",
    "        \n",
    "        objectives[epoch] = train_err / (train_batches + 1)\n",
    "        \n",
    "    plt.title('Learning Curve')\n",
    "    plt.plot(objectives)\n",
    "    plt.xlabel('Num Epoch')\n",
    "    plt.ylabel('Cross Entropy')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train_target = pd.read_csv('Ytr.csv', sep = ',', header = None, skiprows=1)\n",
    "df_train = pd.read_csv('Xtr.csv', sep = ',', header = None)\n",
    "df_test = pd.read_csv('Xte.csv', sep = ',', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>3063</th>\n",
       "      <th>3064</th>\n",
       "      <th>3065</th>\n",
       "      <th>3066</th>\n",
       "      <th>3067</th>\n",
       "      <th>3068</th>\n",
       "      <th>3069</th>\n",
       "      <th>3070</th>\n",
       "      <th>3071</th>\n",
       "      <th>3072</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.021317</td>\n",
       "      <td>-0.029188</td>\n",
       "      <td>-0.021746</td>\n",
       "      <td>-0.029482</td>\n",
       "      <td>0.011179</td>\n",
       "      <td>0.032251</td>\n",
       "      <td>0.013757</td>\n",
       "      <td>0.015677</td>\n",
       "      <td>-0.049118</td>\n",
       "      <td>-0.051430</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013725</td>\n",
       "      <td>0.123213</td>\n",
       "      <td>-0.006638</td>\n",
       "      <td>-0.043762</td>\n",
       "      <td>-0.011082</td>\n",
       "      <td>-0.003506</td>\n",
       "      <td>-0.007296</td>\n",
       "      <td>-0.020889</td>\n",
       "      <td>-0.024223</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.038809</td>\n",
       "      <td>0.017464</td>\n",
       "      <td>0.033951</td>\n",
       "      <td>0.062252</td>\n",
       "      <td>0.051475</td>\n",
       "      <td>0.071586</td>\n",
       "      <td>0.076982</td>\n",
       "      <td>0.078035</td>\n",
       "      <td>0.058132</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010371</td>\n",
       "      <td>-0.016821</td>\n",
       "      <td>0.008783</td>\n",
       "      <td>-0.007583</td>\n",
       "      <td>-0.024359</td>\n",
       "      <td>-0.002662</td>\n",
       "      <td>-0.003553</td>\n",
       "      <td>-0.000605</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.013505</td>\n",
       "      <td>-0.040433</td>\n",
       "      <td>-0.023872</td>\n",
       "      <td>-0.017034</td>\n",
       "      <td>0.005898</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>-0.005875</td>\n",
       "      <td>-0.028056</td>\n",
       "      <td>-0.005516</td>\n",
       "      <td>-0.025364</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000917</td>\n",
       "      <td>0.008904</td>\n",
       "      <td>0.010832</td>\n",
       "      <td>-0.000609</td>\n",
       "      <td>0.001183</td>\n",
       "      <td>0.000750</td>\n",
       "      <td>-0.006280</td>\n",
       "      <td>0.018178</td>\n",
       "      <td>0.009699</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.079833</td>\n",
       "      <td>0.087111</td>\n",
       "      <td>0.095980</td>\n",
       "      <td>0.085299</td>\n",
       "      <td>0.090196</td>\n",
       "      <td>0.092923</td>\n",
       "      <td>0.085450</td>\n",
       "      <td>0.093836</td>\n",
       "      <td>0.089580</td>\n",
       "      <td>0.089868</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003782</td>\n",
       "      <td>0.015810</td>\n",
       "      <td>0.006891</td>\n",
       "      <td>-0.001391</td>\n",
       "      <td>0.005947</td>\n",
       "      <td>0.015124</td>\n",
       "      <td>0.012474</td>\n",
       "      <td>-0.013197</td>\n",
       "      <td>-0.037887</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.003421</td>\n",
       "      <td>-0.012074</td>\n",
       "      <td>0.003435</td>\n",
       "      <td>0.002495</td>\n",
       "      <td>0.017810</td>\n",
       "      <td>0.009306</td>\n",
       "      <td>-0.011229</td>\n",
       "      <td>-0.040445</td>\n",
       "      <td>-0.028034</td>\n",
       "      <td>0.059742</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004841</td>\n",
       "      <td>0.013999</td>\n",
       "      <td>0.020970</td>\n",
       "      <td>0.043955</td>\n",
       "      <td>0.026964</td>\n",
       "      <td>0.030705</td>\n",
       "      <td>-0.002225</td>\n",
       "      <td>0.009616</td>\n",
       "      <td>0.006040</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.002235</td>\n",
       "      <td>-0.001436</td>\n",
       "      <td>-0.001510</td>\n",
       "      <td>0.005397</td>\n",
       "      <td>-0.001300</td>\n",
       "      <td>-0.000326</td>\n",
       "      <td>-0.000283</td>\n",
       "      <td>-0.000483</td>\n",
       "      <td>-0.000515</td>\n",
       "      <td>-0.000552</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.002762</td>\n",
       "      <td>-0.004692</td>\n",
       "      <td>-0.004202</td>\n",
       "      <td>-0.003634</td>\n",
       "      <td>-0.000311</td>\n",
       "      <td>-0.004221</td>\n",
       "      <td>-0.003897</td>\n",
       "      <td>0.002847</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.013060</td>\n",
       "      <td>-0.009028</td>\n",
       "      <td>-0.021637</td>\n",
       "      <td>-0.022661</td>\n",
       "      <td>-0.025891</td>\n",
       "      <td>-0.022931</td>\n",
       "      <td>-0.024733</td>\n",
       "      <td>-0.033114</td>\n",
       "      <td>-0.031494</td>\n",
       "      <td>-0.038910</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006623</td>\n",
       "      <td>-0.002786</td>\n",
       "      <td>-0.002075</td>\n",
       "      <td>-0.004846</td>\n",
       "      <td>-0.002113</td>\n",
       "      <td>-0.006232</td>\n",
       "      <td>-0.014587</td>\n",
       "      <td>-0.076449</td>\n",
       "      <td>-0.031167</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.010942</td>\n",
       "      <td>0.006328</td>\n",
       "      <td>0.002636</td>\n",
       "      <td>0.006784</td>\n",
       "      <td>0.008367</td>\n",
       "      <td>0.001298</td>\n",
       "      <td>0.006004</td>\n",
       "      <td>0.003266</td>\n",
       "      <td>-0.004742</td>\n",
       "      <td>-0.006500</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005236</td>\n",
       "      <td>-0.001792</td>\n",
       "      <td>-0.003260</td>\n",
       "      <td>-0.003570</td>\n",
       "      <td>-0.001974</td>\n",
       "      <td>0.008249</td>\n",
       "      <td>-0.001909</td>\n",
       "      <td>-0.002572</td>\n",
       "      <td>-0.002357</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.010323</td>\n",
       "      <td>-0.000446</td>\n",
       "      <td>-0.028734</td>\n",
       "      <td>0.002662</td>\n",
       "      <td>0.003080</td>\n",
       "      <td>-0.016970</td>\n",
       "      <td>-0.023255</td>\n",
       "      <td>-0.032693</td>\n",
       "      <td>-0.025744</td>\n",
       "      <td>0.009961</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003864</td>\n",
       "      <td>-0.002841</td>\n",
       "      <td>-0.015710</td>\n",
       "      <td>-0.013947</td>\n",
       "      <td>0.006856</td>\n",
       "      <td>0.030468</td>\n",
       "      <td>0.056211</td>\n",
       "      <td>0.056140</td>\n",
       "      <td>0.060534</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.033041</td>\n",
       "      <td>0.022215</td>\n",
       "      <td>0.017222</td>\n",
       "      <td>0.023705</td>\n",
       "      <td>0.027109</td>\n",
       "      <td>0.033807</td>\n",
       "      <td>0.018699</td>\n",
       "      <td>0.017393</td>\n",
       "      <td>0.029579</td>\n",
       "      <td>0.020493</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032927</td>\n",
       "      <td>0.020618</td>\n",
       "      <td>-0.002640</td>\n",
       "      <td>0.008354</td>\n",
       "      <td>-0.005770</td>\n",
       "      <td>0.008310</td>\n",
       "      <td>-0.005025</td>\n",
       "      <td>-0.000503</td>\n",
       "      <td>-0.004402</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.031338</td>\n",
       "      <td>-0.053711</td>\n",
       "      <td>-0.022662</td>\n",
       "      <td>-0.010457</td>\n",
       "      <td>0.005947</td>\n",
       "      <td>0.031033</td>\n",
       "      <td>0.052270</td>\n",
       "      <td>0.068455</td>\n",
       "      <td>0.077711</td>\n",
       "      <td>0.066538</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009768</td>\n",
       "      <td>0.006889</td>\n",
       "      <td>0.005848</td>\n",
       "      <td>0.009870</td>\n",
       "      <td>0.010684</td>\n",
       "      <td>0.018351</td>\n",
       "      <td>0.018319</td>\n",
       "      <td>0.007916</td>\n",
       "      <td>-0.010940</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.016557</td>\n",
       "      <td>0.009153</td>\n",
       "      <td>0.007265</td>\n",
       "      <td>0.007597</td>\n",
       "      <td>0.014157</td>\n",
       "      <td>0.033290</td>\n",
       "      <td>0.050623</td>\n",
       "      <td>0.051277</td>\n",
       "      <td>0.033961</td>\n",
       "      <td>0.055225</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003214</td>\n",
       "      <td>-0.009795</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>0.003709</td>\n",
       "      <td>0.016094</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.005235</td>\n",
       "      <td>0.036845</td>\n",
       "      <td>-0.009058</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.078333</td>\n",
       "      <td>-0.039400</td>\n",
       "      <td>-0.035478</td>\n",
       "      <td>-0.015266</td>\n",
       "      <td>-0.015293</td>\n",
       "      <td>-0.003589</td>\n",
       "      <td>0.003317</td>\n",
       "      <td>-0.008636</td>\n",
       "      <td>-0.035172</td>\n",
       "      <td>-0.028796</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011291</td>\n",
       "      <td>0.004020</td>\n",
       "      <td>0.008909</td>\n",
       "      <td>0.009295</td>\n",
       "      <td>-0.003402</td>\n",
       "      <td>-0.005555</td>\n",
       "      <td>0.009537</td>\n",
       "      <td>0.016247</td>\n",
       "      <td>0.019649</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.013076</td>\n",
       "      <td>0.015101</td>\n",
       "      <td>0.027279</td>\n",
       "      <td>0.005082</td>\n",
       "      <td>0.014445</td>\n",
       "      <td>0.000578</td>\n",
       "      <td>0.012420</td>\n",
       "      <td>0.024485</td>\n",
       "      <td>-0.015884</td>\n",
       "      <td>-0.034665</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005504</td>\n",
       "      <td>-0.013007</td>\n",
       "      <td>-0.046126</td>\n",
       "      <td>-0.012297</td>\n",
       "      <td>0.042970</td>\n",
       "      <td>0.033769</td>\n",
       "      <td>0.005311</td>\n",
       "      <td>-0.080462</td>\n",
       "      <td>0.036623</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.002029</td>\n",
       "      <td>-0.006340</td>\n",
       "      <td>-0.004841</td>\n",
       "      <td>-0.004535</td>\n",
       "      <td>-0.001540</td>\n",
       "      <td>-0.002548</td>\n",
       "      <td>-0.002430</td>\n",
       "      <td>-0.002305</td>\n",
       "      <td>-0.002202</td>\n",
       "      <td>-0.002327</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.002047</td>\n",
       "      <td>-0.001220</td>\n",
       "      <td>0.000749</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000468</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.128042</td>\n",
       "      <td>0.006306</td>\n",
       "      <td>0.063154</td>\n",
       "      <td>0.049963</td>\n",
       "      <td>0.035655</td>\n",
       "      <td>0.024088</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>-0.005964</td>\n",
       "      <td>-0.028265</td>\n",
       "      <td>-0.069240</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.106236</td>\n",
       "      <td>0.043024</td>\n",
       "      <td>0.026064</td>\n",
       "      <td>-0.070579</td>\n",
       "      <td>-0.080341</td>\n",
       "      <td>-0.062883</td>\n",
       "      <td>-0.011459</td>\n",
       "      <td>0.013694</td>\n",
       "      <td>0.003556</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043850</td>\n",
       "      <td>0.035850</td>\n",
       "      <td>0.025108</td>\n",
       "      <td>0.019560</td>\n",
       "      <td>0.022995</td>\n",
       "      <td>-0.000538</td>\n",
       "      <td>0.017666</td>\n",
       "      <td>0.027076</td>\n",
       "      <td>0.009692</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.013313</td>\n",
       "      <td>-0.020226</td>\n",
       "      <td>-0.024988</td>\n",
       "      <td>-0.017642</td>\n",
       "      <td>0.075197</td>\n",
       "      <td>-0.016114</td>\n",
       "      <td>-0.011271</td>\n",
       "      <td>0.006715</td>\n",
       "      <td>-0.002664</td>\n",
       "      <td>0.003054</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016604</td>\n",
       "      <td>0.016532</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>-0.010608</td>\n",
       "      <td>-0.015623</td>\n",
       "      <td>-0.045108</td>\n",
       "      <td>-0.082262</td>\n",
       "      <td>-0.099732</td>\n",
       "      <td>-0.011712</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.020948</td>\n",
       "      <td>-0.018588</td>\n",
       "      <td>-0.026161</td>\n",
       "      <td>-0.005675</td>\n",
       "      <td>-0.001587</td>\n",
       "      <td>-0.002780</td>\n",
       "      <td>-0.003908</td>\n",
       "      <td>-0.003197</td>\n",
       "      <td>-0.003252</td>\n",
       "      <td>-0.005406</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010153</td>\n",
       "      <td>-0.001188</td>\n",
       "      <td>-0.009929</td>\n",
       "      <td>-0.012815</td>\n",
       "      <td>-0.001753</td>\n",
       "      <td>0.004605</td>\n",
       "      <td>0.001046</td>\n",
       "      <td>0.003289</td>\n",
       "      <td>0.004990</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.013797</td>\n",
       "      <td>-0.036495</td>\n",
       "      <td>-0.014289</td>\n",
       "      <td>0.018623</td>\n",
       "      <td>-0.017866</td>\n",
       "      <td>-0.006413</td>\n",
       "      <td>0.006277</td>\n",
       "      <td>0.006650</td>\n",
       "      <td>-0.007309</td>\n",
       "      <td>-0.007378</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019849</td>\n",
       "      <td>-0.029290</td>\n",
       "      <td>-0.042688</td>\n",
       "      <td>0.001996</td>\n",
       "      <td>0.045124</td>\n",
       "      <td>0.032707</td>\n",
       "      <td>-0.004925</td>\n",
       "      <td>0.040127</td>\n",
       "      <td>0.009098</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.015284</td>\n",
       "      <td>0.012368</td>\n",
       "      <td>0.024529</td>\n",
       "      <td>0.023183</td>\n",
       "      <td>0.010369</td>\n",
       "      <td>0.011631</td>\n",
       "      <td>0.012967</td>\n",
       "      <td>0.006176</td>\n",
       "      <td>0.004024</td>\n",
       "      <td>0.001508</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.043750</td>\n",
       "      <td>-0.002809</td>\n",
       "      <td>-0.013538</td>\n",
       "      <td>-0.012697</td>\n",
       "      <td>-0.010171</td>\n",
       "      <td>-0.009076</td>\n",
       "      <td>-0.008570</td>\n",
       "      <td>-0.002556</td>\n",
       "      <td>-0.001947</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.004894</td>\n",
       "      <td>-0.002317</td>\n",
       "      <td>0.000647</td>\n",
       "      <td>0.001272</td>\n",
       "      <td>0.003164</td>\n",
       "      <td>0.004028</td>\n",
       "      <td>0.006231</td>\n",
       "      <td>0.004969</td>\n",
       "      <td>0.002798</td>\n",
       "      <td>0.001769</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008992</td>\n",
       "      <td>-0.012087</td>\n",
       "      <td>0.006003</td>\n",
       "      <td>0.026791</td>\n",
       "      <td>-0.003236</td>\n",
       "      <td>-0.005615</td>\n",
       "      <td>0.008982</td>\n",
       "      <td>-0.014955</td>\n",
       "      <td>-0.012604</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.003639</td>\n",
       "      <td>0.003092</td>\n",
       "      <td>0.002663</td>\n",
       "      <td>-0.002966</td>\n",
       "      <td>-0.001633</td>\n",
       "      <td>-0.000295</td>\n",
       "      <td>-0.002939</td>\n",
       "      <td>0.003335</td>\n",
       "      <td>-0.000117</td>\n",
       "      <td>-0.028369</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009058</td>\n",
       "      <td>-0.020949</td>\n",
       "      <td>-0.022272</td>\n",
       "      <td>0.010033</td>\n",
       "      <td>0.007964</td>\n",
       "      <td>0.006688</td>\n",
       "      <td>-0.024299</td>\n",
       "      <td>-0.015483</td>\n",
       "      <td>0.018256</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.001603</td>\n",
       "      <td>0.003666</td>\n",
       "      <td>0.001969</td>\n",
       "      <td>0.001370</td>\n",
       "      <td>0.002672</td>\n",
       "      <td>-0.025949</td>\n",
       "      <td>-0.010128</td>\n",
       "      <td>0.192425</td>\n",
       "      <td>0.056936</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002019</td>\n",
       "      <td>0.001963</td>\n",
       "      <td>0.021156</td>\n",
       "      <td>0.013631</td>\n",
       "      <td>-0.027612</td>\n",
       "      <td>-0.024798</td>\n",
       "      <td>0.008288</td>\n",
       "      <td>0.008967</td>\n",
       "      <td>-0.026417</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.001468</td>\n",
       "      <td>-0.007685</td>\n",
       "      <td>-0.002316</td>\n",
       "      <td>0.009876</td>\n",
       "      <td>0.001603</td>\n",
       "      <td>-0.000779</td>\n",
       "      <td>-0.002800</td>\n",
       "      <td>-0.002749</td>\n",
       "      <td>0.004483</td>\n",
       "      <td>-0.000274</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007260</td>\n",
       "      <td>0.010406</td>\n",
       "      <td>0.002906</td>\n",
       "      <td>0.020847</td>\n",
       "      <td>0.012062</td>\n",
       "      <td>0.003060</td>\n",
       "      <td>0.012369</td>\n",
       "      <td>0.010780</td>\n",
       "      <td>-0.002764</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.079307</td>\n",
       "      <td>-0.044940</td>\n",
       "      <td>-0.004161</td>\n",
       "      <td>-0.000975</td>\n",
       "      <td>-0.000746</td>\n",
       "      <td>-0.002121</td>\n",
       "      <td>0.040337</td>\n",
       "      <td>-0.035029</td>\n",
       "      <td>-0.088008</td>\n",
       "      <td>0.022112</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010164</td>\n",
       "      <td>-0.009338</td>\n",
       "      <td>0.004636</td>\n",
       "      <td>0.005752</td>\n",
       "      <td>-0.010149</td>\n",
       "      <td>-0.021101</td>\n",
       "      <td>-0.006629</td>\n",
       "      <td>0.012045</td>\n",
       "      <td>0.041208</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.044073</td>\n",
       "      <td>0.037817</td>\n",
       "      <td>0.023229</td>\n",
       "      <td>0.023214</td>\n",
       "      <td>0.052923</td>\n",
       "      <td>0.068834</td>\n",
       "      <td>0.099976</td>\n",
       "      <td>0.119910</td>\n",
       "      <td>0.124708</td>\n",
       "      <td>0.129756</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100456</td>\n",
       "      <td>0.080219</td>\n",
       "      <td>0.058940</td>\n",
       "      <td>0.037133</td>\n",
       "      <td>0.039924</td>\n",
       "      <td>0.045988</td>\n",
       "      <td>0.035910</td>\n",
       "      <td>0.013805</td>\n",
       "      <td>0.035574</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.002332</td>\n",
       "      <td>-0.014680</td>\n",
       "      <td>-0.030029</td>\n",
       "      <td>-0.045095</td>\n",
       "      <td>-0.024173</td>\n",
       "      <td>-0.009167</td>\n",
       "      <td>-0.000573</td>\n",
       "      <td>-0.011225</td>\n",
       "      <td>-0.008913</td>\n",
       "      <td>-0.008394</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001986</td>\n",
       "      <td>-0.001504</td>\n",
       "      <td>0.003195</td>\n",
       "      <td>0.004896</td>\n",
       "      <td>-0.001716</td>\n",
       "      <td>-0.003208</td>\n",
       "      <td>-0.003658</td>\n",
       "      <td>0.001874</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.005987</td>\n",
       "      <td>0.002439</td>\n",
       "      <td>0.006310</td>\n",
       "      <td>-0.000734</td>\n",
       "      <td>0.002256</td>\n",
       "      <td>0.009119</td>\n",
       "      <td>0.009968</td>\n",
       "      <td>0.006015</td>\n",
       "      <td>0.005826</td>\n",
       "      <td>0.003416</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008532</td>\n",
       "      <td>0.003724</td>\n",
       "      <td>0.025260</td>\n",
       "      <td>0.005692</td>\n",
       "      <td>-0.004188</td>\n",
       "      <td>-0.016492</td>\n",
       "      <td>-0.008390</td>\n",
       "      <td>-0.009993</td>\n",
       "      <td>0.006029</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.000655</td>\n",
       "      <td>-0.001665</td>\n",
       "      <td>0.051067</td>\n",
       "      <td>0.020941</td>\n",
       "      <td>0.043685</td>\n",
       "      <td>0.081384</td>\n",
       "      <td>0.023919</td>\n",
       "      <td>-0.008496</td>\n",
       "      <td>-0.062231</td>\n",
       "      <td>0.004672</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019435</td>\n",
       "      <td>-0.043152</td>\n",
       "      <td>-0.053377</td>\n",
       "      <td>-0.043291</td>\n",
       "      <td>-0.042670</td>\n",
       "      <td>-0.008868</td>\n",
       "      <td>0.013367</td>\n",
       "      <td>0.011265</td>\n",
       "      <td>0.004730</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970</th>\n",
       "      <td>-0.049367</td>\n",
       "      <td>-0.023214</td>\n",
       "      <td>-0.012704</td>\n",
       "      <td>-0.010242</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.023740</td>\n",
       "      <td>0.023142</td>\n",
       "      <td>-0.096094</td>\n",
       "      <td>-0.009804</td>\n",
       "      <td>0.089443</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017684</td>\n",
       "      <td>0.027654</td>\n",
       "      <td>0.070135</td>\n",
       "      <td>0.047392</td>\n",
       "      <td>0.068083</td>\n",
       "      <td>0.104659</td>\n",
       "      <td>0.005392</td>\n",
       "      <td>-0.015622</td>\n",
       "      <td>-0.067773</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1971</th>\n",
       "      <td>0.004169</td>\n",
       "      <td>0.004311</td>\n",
       "      <td>0.000280</td>\n",
       "      <td>0.024825</td>\n",
       "      <td>0.001135</td>\n",
       "      <td>-0.103117</td>\n",
       "      <td>-0.138674</td>\n",
       "      <td>0.015781</td>\n",
       "      <td>0.016727</td>\n",
       "      <td>0.002762</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004385</td>\n",
       "      <td>0.004299</td>\n",
       "      <td>-0.000568</td>\n",
       "      <td>0.001926</td>\n",
       "      <td>-0.002688</td>\n",
       "      <td>0.000741</td>\n",
       "      <td>0.000268</td>\n",
       "      <td>-0.000509</td>\n",
       "      <td>-0.000291</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1972</th>\n",
       "      <td>-0.019942</td>\n",
       "      <td>-0.023973</td>\n",
       "      <td>-0.021969</td>\n",
       "      <td>-0.012402</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>0.002875</td>\n",
       "      <td>-0.009791</td>\n",
       "      <td>-0.029427</td>\n",
       "      <td>-0.042189</td>\n",
       "      <td>-0.018761</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011086</td>\n",
       "      <td>0.012365</td>\n",
       "      <td>-0.017993</td>\n",
       "      <td>-0.001214</td>\n",
       "      <td>-0.013841</td>\n",
       "      <td>0.018112</td>\n",
       "      <td>-0.006222</td>\n",
       "      <td>-0.004652</td>\n",
       "      <td>0.026367</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1973</th>\n",
       "      <td>0.029817</td>\n",
       "      <td>0.028034</td>\n",
       "      <td>-0.001200</td>\n",
       "      <td>-0.012980</td>\n",
       "      <td>-0.035495</td>\n",
       "      <td>-0.089925</td>\n",
       "      <td>-0.080743</td>\n",
       "      <td>-0.072421</td>\n",
       "      <td>-0.047086</td>\n",
       "      <td>-0.040740</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003222</td>\n",
       "      <td>-0.012463</td>\n",
       "      <td>-0.003267</td>\n",
       "      <td>0.013649</td>\n",
       "      <td>0.024968</td>\n",
       "      <td>0.044639</td>\n",
       "      <td>-0.006488</td>\n",
       "      <td>-0.042983</td>\n",
       "      <td>-0.001836</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1974</th>\n",
       "      <td>-0.000207</td>\n",
       "      <td>0.001634</td>\n",
       "      <td>0.002909</td>\n",
       "      <td>0.002384</td>\n",
       "      <td>0.008625</td>\n",
       "      <td>0.023292</td>\n",
       "      <td>0.019564</td>\n",
       "      <td>0.015520</td>\n",
       "      <td>0.004113</td>\n",
       "      <td>-0.013274</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001673</td>\n",
       "      <td>0.000690</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>-0.000064</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.001722</td>\n",
       "      <td>0.001723</td>\n",
       "      <td>-0.005875</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1975</th>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.000599</td>\n",
       "      <td>0.000710</td>\n",
       "      <td>0.001807</td>\n",
       "      <td>0.003740</td>\n",
       "      <td>0.008266</td>\n",
       "      <td>0.012714</td>\n",
       "      <td>0.016753</td>\n",
       "      <td>0.025388</td>\n",
       "      <td>0.027456</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003038</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.001108</td>\n",
       "      <td>0.000733</td>\n",
       "      <td>0.000687</td>\n",
       "      <td>0.000645</td>\n",
       "      <td>0.000628</td>\n",
       "      <td>0.000573</td>\n",
       "      <td>0.000565</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1976</th>\n",
       "      <td>0.014453</td>\n",
       "      <td>0.006857</td>\n",
       "      <td>0.000321</td>\n",
       "      <td>0.008958</td>\n",
       "      <td>0.014514</td>\n",
       "      <td>0.009214</td>\n",
       "      <td>0.005750</td>\n",
       "      <td>0.002227</td>\n",
       "      <td>0.006196</td>\n",
       "      <td>-0.004517</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020406</td>\n",
       "      <td>0.028140</td>\n",
       "      <td>0.016670</td>\n",
       "      <td>0.012407</td>\n",
       "      <td>0.022417</td>\n",
       "      <td>0.007028</td>\n",
       "      <td>-0.000088</td>\n",
       "      <td>0.018758</td>\n",
       "      <td>-0.005744</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977</th>\n",
       "      <td>-0.066317</td>\n",
       "      <td>-0.029153</td>\n",
       "      <td>0.046721</td>\n",
       "      <td>0.053436</td>\n",
       "      <td>0.012910</td>\n",
       "      <td>-0.035238</td>\n",
       "      <td>-0.002804</td>\n",
       "      <td>0.032899</td>\n",
       "      <td>0.074104</td>\n",
       "      <td>0.076151</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014913</td>\n",
       "      <td>0.000404</td>\n",
       "      <td>-0.001300</td>\n",
       "      <td>0.001271</td>\n",
       "      <td>-0.027878</td>\n",
       "      <td>0.016780</td>\n",
       "      <td>0.038456</td>\n",
       "      <td>-0.005518</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1978</th>\n",
       "      <td>-0.000708</td>\n",
       "      <td>-0.031648</td>\n",
       "      <td>-0.028026</td>\n",
       "      <td>-0.010788</td>\n",
       "      <td>-0.023476</td>\n",
       "      <td>-0.009266</td>\n",
       "      <td>-0.014904</td>\n",
       "      <td>-0.047553</td>\n",
       "      <td>-0.022767</td>\n",
       "      <td>0.011040</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031812</td>\n",
       "      <td>-0.005208</td>\n",
       "      <td>0.026227</td>\n",
       "      <td>0.008688</td>\n",
       "      <td>-0.075301</td>\n",
       "      <td>0.012872</td>\n",
       "      <td>0.007400</td>\n",
       "      <td>-0.054902</td>\n",
       "      <td>-0.085861</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979</th>\n",
       "      <td>0.010402</td>\n",
       "      <td>0.004792</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>-0.004293</td>\n",
       "      <td>-0.006872</td>\n",
       "      <td>-0.004842</td>\n",
       "      <td>-0.001236</td>\n",
       "      <td>-0.037958</td>\n",
       "      <td>-0.043809</td>\n",
       "      <td>-0.028412</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012415</td>\n",
       "      <td>-0.020330</td>\n",
       "      <td>-0.020171</td>\n",
       "      <td>-0.011733</td>\n",
       "      <td>-0.009458</td>\n",
       "      <td>-0.003552</td>\n",
       "      <td>-0.007833</td>\n",
       "      <td>-0.007810</td>\n",
       "      <td>-0.031812</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>-0.000063</td>\n",
       "      <td>-0.000736</td>\n",
       "      <td>-0.001383</td>\n",
       "      <td>-0.000466</td>\n",
       "      <td>-0.005586</td>\n",
       "      <td>0.006051</td>\n",
       "      <td>0.001748</td>\n",
       "      <td>-0.013153</td>\n",
       "      <td>0.004683</td>\n",
       "      <td>-0.000745</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002964</td>\n",
       "      <td>-0.039568</td>\n",
       "      <td>-0.032793</td>\n",
       "      <td>-0.004230</td>\n",
       "      <td>0.028499</td>\n",
       "      <td>0.024672</td>\n",
       "      <td>0.031623</td>\n",
       "      <td>0.031356</td>\n",
       "      <td>0.030750</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981</th>\n",
       "      <td>-0.014033</td>\n",
       "      <td>0.018120</td>\n",
       "      <td>-0.004490</td>\n",
       "      <td>-0.028622</td>\n",
       "      <td>0.018973</td>\n",
       "      <td>0.006707</td>\n",
       "      <td>-0.035800</td>\n",
       "      <td>-0.009398</td>\n",
       "      <td>0.018722</td>\n",
       "      <td>-0.004503</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031860</td>\n",
       "      <td>0.060749</td>\n",
       "      <td>0.026839</td>\n",
       "      <td>-0.001880</td>\n",
       "      <td>-0.005826</td>\n",
       "      <td>-0.006457</td>\n",
       "      <td>0.012299</td>\n",
       "      <td>0.010584</td>\n",
       "      <td>-0.001859</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982</th>\n",
       "      <td>0.014439</td>\n",
       "      <td>0.013683</td>\n",
       "      <td>0.009910</td>\n",
       "      <td>0.010641</td>\n",
       "      <td>0.006615</td>\n",
       "      <td>0.001565</td>\n",
       "      <td>0.001791</td>\n",
       "      <td>-0.021253</td>\n",
       "      <td>-0.037579</td>\n",
       "      <td>-0.060752</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033646</td>\n",
       "      <td>-0.023532</td>\n",
       "      <td>-0.019896</td>\n",
       "      <td>-0.022324</td>\n",
       "      <td>-0.012913</td>\n",
       "      <td>-0.026536</td>\n",
       "      <td>-0.033833</td>\n",
       "      <td>-0.024506</td>\n",
       "      <td>-0.024253</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983</th>\n",
       "      <td>0.007101</td>\n",
       "      <td>0.000566</td>\n",
       "      <td>0.001267</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.002923</td>\n",
       "      <td>0.003814</td>\n",
       "      <td>0.002789</td>\n",
       "      <td>0.002874</td>\n",
       "      <td>0.002749</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002502</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>-0.000098</td>\n",
       "      <td>-0.001410</td>\n",
       "      <td>0.001013</td>\n",
       "      <td>0.000798</td>\n",
       "      <td>0.001095</td>\n",
       "      <td>0.001282</td>\n",
       "      <td>-0.000330</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984</th>\n",
       "      <td>-0.007241</td>\n",
       "      <td>-0.007990</td>\n",
       "      <td>-0.024073</td>\n",
       "      <td>-0.017647</td>\n",
       "      <td>-0.012083</td>\n",
       "      <td>-0.005893</td>\n",
       "      <td>-0.002557</td>\n",
       "      <td>-0.004222</td>\n",
       "      <td>-0.008775</td>\n",
       "      <td>-0.012841</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022167</td>\n",
       "      <td>0.021740</td>\n",
       "      <td>0.015257</td>\n",
       "      <td>-0.046881</td>\n",
       "      <td>-0.065418</td>\n",
       "      <td>-0.014639</td>\n",
       "      <td>0.022241</td>\n",
       "      <td>-0.039683</td>\n",
       "      <td>-0.053298</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985</th>\n",
       "      <td>-0.043725</td>\n",
       "      <td>-0.020229</td>\n",
       "      <td>0.002720</td>\n",
       "      <td>-0.021290</td>\n",
       "      <td>-0.050453</td>\n",
       "      <td>-0.048004</td>\n",
       "      <td>-0.004628</td>\n",
       "      <td>0.051912</td>\n",
       "      <td>0.086225</td>\n",
       "      <td>0.114307</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.058727</td>\n",
       "      <td>-0.055159</td>\n",
       "      <td>-0.062923</td>\n",
       "      <td>-0.064047</td>\n",
       "      <td>-0.068489</td>\n",
       "      <td>-0.069911</td>\n",
       "      <td>-0.071826</td>\n",
       "      <td>-0.069520</td>\n",
       "      <td>-0.074040</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986</th>\n",
       "      <td>0.016831</td>\n",
       "      <td>0.020305</td>\n",
       "      <td>0.010044</td>\n",
       "      <td>0.029877</td>\n",
       "      <td>0.055909</td>\n",
       "      <td>0.073108</td>\n",
       "      <td>0.078796</td>\n",
       "      <td>0.084910</td>\n",
       "      <td>0.069699</td>\n",
       "      <td>0.016255</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014804</td>\n",
       "      <td>0.002547</td>\n",
       "      <td>0.010930</td>\n",
       "      <td>-0.007617</td>\n",
       "      <td>-0.024243</td>\n",
       "      <td>-0.002141</td>\n",
       "      <td>0.002854</td>\n",
       "      <td>0.018120</td>\n",
       "      <td>0.047070</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987</th>\n",
       "      <td>0.111790</td>\n",
       "      <td>0.084298</td>\n",
       "      <td>0.091169</td>\n",
       "      <td>0.074928</td>\n",
       "      <td>0.087373</td>\n",
       "      <td>0.024236</td>\n",
       "      <td>-0.190039</td>\n",
       "      <td>0.012912</td>\n",
       "      <td>-0.002945</td>\n",
       "      <td>-0.066698</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032753</td>\n",
       "      <td>0.027317</td>\n",
       "      <td>0.010146</td>\n",
       "      <td>0.004048</td>\n",
       "      <td>0.014203</td>\n",
       "      <td>-0.017538</td>\n",
       "      <td>0.012408</td>\n",
       "      <td>0.057850</td>\n",
       "      <td>0.028633</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988</th>\n",
       "      <td>-0.003241</td>\n",
       "      <td>0.004197</td>\n",
       "      <td>0.004403</td>\n",
       "      <td>0.004030</td>\n",
       "      <td>0.003849</td>\n",
       "      <td>0.003754</td>\n",
       "      <td>0.003653</td>\n",
       "      <td>0.005744</td>\n",
       "      <td>-0.000257</td>\n",
       "      <td>0.003945</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052756</td>\n",
       "      <td>0.049592</td>\n",
       "      <td>0.048908</td>\n",
       "      <td>0.051547</td>\n",
       "      <td>0.047621</td>\n",
       "      <td>0.041506</td>\n",
       "      <td>0.044924</td>\n",
       "      <td>0.040239</td>\n",
       "      <td>0.010878</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989</th>\n",
       "      <td>0.009668</td>\n",
       "      <td>-0.004713</td>\n",
       "      <td>0.003391</td>\n",
       "      <td>0.001080</td>\n",
       "      <td>-0.054880</td>\n",
       "      <td>-0.064890</td>\n",
       "      <td>-0.025821</td>\n",
       "      <td>-0.066461</td>\n",
       "      <td>-0.104204</td>\n",
       "      <td>-0.053223</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007417</td>\n",
       "      <td>-0.021339</td>\n",
       "      <td>-0.011602</td>\n",
       "      <td>0.004651</td>\n",
       "      <td>-0.001503</td>\n",
       "      <td>-0.009852</td>\n",
       "      <td>-0.031243</td>\n",
       "      <td>0.045586</td>\n",
       "      <td>0.046126</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>-0.000440</td>\n",
       "      <td>-0.040486</td>\n",
       "      <td>-0.004746</td>\n",
       "      <td>0.019339</td>\n",
       "      <td>-0.011171</td>\n",
       "      <td>-0.023570</td>\n",
       "      <td>0.005669</td>\n",
       "      <td>0.001446</td>\n",
       "      <td>0.000790</td>\n",
       "      <td>-0.001433</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047149</td>\n",
       "      <td>0.032900</td>\n",
       "      <td>0.019979</td>\n",
       "      <td>0.010586</td>\n",
       "      <td>-0.003565</td>\n",
       "      <td>-0.006615</td>\n",
       "      <td>-0.004334</td>\n",
       "      <td>-0.002753</td>\n",
       "      <td>-0.004215</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991</th>\n",
       "      <td>-0.006729</td>\n",
       "      <td>0.011705</td>\n",
       "      <td>0.047846</td>\n",
       "      <td>0.014878</td>\n",
       "      <td>0.005887</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.010107</td>\n",
       "      <td>-0.002872</td>\n",
       "      <td>-0.001249</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>...</td>\n",
       "      <td>0.067558</td>\n",
       "      <td>0.082918</td>\n",
       "      <td>0.072749</td>\n",
       "      <td>0.048206</td>\n",
       "      <td>0.044773</td>\n",
       "      <td>0.065448</td>\n",
       "      <td>0.060457</td>\n",
       "      <td>0.008385</td>\n",
       "      <td>-0.001930</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992</th>\n",
       "      <td>-0.097138</td>\n",
       "      <td>-0.079585</td>\n",
       "      <td>-0.059086</td>\n",
       "      <td>-0.052043</td>\n",
       "      <td>-0.034967</td>\n",
       "      <td>-0.019995</td>\n",
       "      <td>0.004845</td>\n",
       "      <td>0.012336</td>\n",
       "      <td>0.014885</td>\n",
       "      <td>0.018788</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010546</td>\n",
       "      <td>0.001651</td>\n",
       "      <td>0.006047</td>\n",
       "      <td>0.011601</td>\n",
       "      <td>0.011044</td>\n",
       "      <td>-0.006630</td>\n",
       "      <td>-0.006376</td>\n",
       "      <td>0.003645</td>\n",
       "      <td>0.006604</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993</th>\n",
       "      <td>-0.093245</td>\n",
       "      <td>-0.058551</td>\n",
       "      <td>0.050890</td>\n",
       "      <td>0.020120</td>\n",
       "      <td>-0.005391</td>\n",
       "      <td>-0.011347</td>\n",
       "      <td>-0.006256</td>\n",
       "      <td>0.017615</td>\n",
       "      <td>0.030588</td>\n",
       "      <td>0.031824</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015452</td>\n",
       "      <td>0.011464</td>\n",
       "      <td>0.006536</td>\n",
       "      <td>0.010482</td>\n",
       "      <td>0.006423</td>\n",
       "      <td>0.001348</td>\n",
       "      <td>-0.001634</td>\n",
       "      <td>0.004288</td>\n",
       "      <td>-0.003357</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994</th>\n",
       "      <td>-0.147514</td>\n",
       "      <td>-0.066310</td>\n",
       "      <td>0.012779</td>\n",
       "      <td>0.001525</td>\n",
       "      <td>0.001099</td>\n",
       "      <td>0.003321</td>\n",
       "      <td>0.004485</td>\n",
       "      <td>-0.001618</td>\n",
       "      <td>0.007575</td>\n",
       "      <td>0.035502</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009267</td>\n",
       "      <td>0.010739</td>\n",
       "      <td>0.015231</td>\n",
       "      <td>0.011451</td>\n",
       "      <td>-0.000737</td>\n",
       "      <td>0.007873</td>\n",
       "      <td>-0.005385</td>\n",
       "      <td>-0.036363</td>\n",
       "      <td>0.028626</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>-0.006021</td>\n",
       "      <td>-0.004997</td>\n",
       "      <td>-0.003527</td>\n",
       "      <td>-0.004530</td>\n",
       "      <td>-0.008682</td>\n",
       "      <td>-0.013278</td>\n",
       "      <td>-0.010563</td>\n",
       "      <td>-0.009648</td>\n",
       "      <td>-0.008027</td>\n",
       "      <td>0.003786</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.044083</td>\n",
       "      <td>-0.041418</td>\n",
       "      <td>-0.042381</td>\n",
       "      <td>-0.063451</td>\n",
       "      <td>-0.012183</td>\n",
       "      <td>-0.045378</td>\n",
       "      <td>-0.076428</td>\n",
       "      <td>0.024682</td>\n",
       "      <td>-0.044529</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>-0.033343</td>\n",
       "      <td>-0.022161</td>\n",
       "      <td>-0.012821</td>\n",
       "      <td>-0.020406</td>\n",
       "      <td>-0.020390</td>\n",
       "      <td>-0.013854</td>\n",
       "      <td>-0.017466</td>\n",
       "      <td>-0.014241</td>\n",
       "      <td>-0.016238</td>\n",
       "      <td>-0.016527</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000734</td>\n",
       "      <td>-0.004880</td>\n",
       "      <td>-0.005046</td>\n",
       "      <td>-0.009637</td>\n",
       "      <td>-0.009953</td>\n",
       "      <td>-0.014024</td>\n",
       "      <td>-0.011671</td>\n",
       "      <td>-0.009456</td>\n",
       "      <td>-0.014382</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>0.022203</td>\n",
       "      <td>0.035480</td>\n",
       "      <td>0.007251</td>\n",
       "      <td>-0.050066</td>\n",
       "      <td>-0.012828</td>\n",
       "      <td>0.041415</td>\n",
       "      <td>0.043532</td>\n",
       "      <td>0.033096</td>\n",
       "      <td>0.042523</td>\n",
       "      <td>-0.014916</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006024</td>\n",
       "      <td>-0.013834</td>\n",
       "      <td>0.009403</td>\n",
       "      <td>-0.005050</td>\n",
       "      <td>-0.018284</td>\n",
       "      <td>0.008724</td>\n",
       "      <td>0.008318</td>\n",
       "      <td>0.000721</td>\n",
       "      <td>-0.002122</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>0.030341</td>\n",
       "      <td>0.014014</td>\n",
       "      <td>0.023996</td>\n",
       "      <td>-0.046175</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>0.036697</td>\n",
       "      <td>-0.069256</td>\n",
       "      <td>-0.063223</td>\n",
       "      <td>0.025327</td>\n",
       "      <td>0.015869</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023376</td>\n",
       "      <td>-0.008562</td>\n",
       "      <td>0.043392</td>\n",
       "      <td>0.043874</td>\n",
       "      <td>0.007881</td>\n",
       "      <td>0.035408</td>\n",
       "      <td>0.015650</td>\n",
       "      <td>0.042028</td>\n",
       "      <td>-0.002508</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>-0.022427</td>\n",
       "      <td>-0.010502</td>\n",
       "      <td>0.006985</td>\n",
       "      <td>-0.004213</td>\n",
       "      <td>-0.007820</td>\n",
       "      <td>-0.001414</td>\n",
       "      <td>0.004402</td>\n",
       "      <td>0.002292</td>\n",
       "      <td>0.004221</td>\n",
       "      <td>0.006314</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040203</td>\n",
       "      <td>0.033803</td>\n",
       "      <td>0.029467</td>\n",
       "      <td>0.008148</td>\n",
       "      <td>-0.001093</td>\n",
       "      <td>-0.000264</td>\n",
       "      <td>0.000986</td>\n",
       "      <td>0.001235</td>\n",
       "      <td>-0.009715</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 3073 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6     \\\n",
       "0     0.021317 -0.029188 -0.021746 -0.029482  0.011179  0.032251  0.013757   \n",
       "1     0.038809  0.017464  0.033951  0.062252  0.051475  0.071586  0.076982   \n",
       "2    -0.013505 -0.040433 -0.023872 -0.017034  0.005898  0.015000 -0.005875   \n",
       "3     0.079833  0.087111  0.095980  0.085299  0.090196  0.092923  0.085450   \n",
       "4     0.003421 -0.012074  0.003435  0.002495  0.017810  0.009306 -0.011229   \n",
       "5     0.002235 -0.001436 -0.001510  0.005397 -0.001300 -0.000326 -0.000283   \n",
       "6     0.013060 -0.009028 -0.021637 -0.022661 -0.025891 -0.022931 -0.024733   \n",
       "7     0.010942  0.006328  0.002636  0.006784  0.008367  0.001298  0.006004   \n",
       "8     0.010323 -0.000446 -0.028734  0.002662  0.003080 -0.016970 -0.023255   \n",
       "9     0.033041  0.022215  0.017222  0.023705  0.027109  0.033807  0.018699   \n",
       "10   -0.031338 -0.053711 -0.022662 -0.010457  0.005947  0.031033  0.052270   \n",
       "11    0.016557  0.009153  0.007265  0.007597  0.014157  0.033290  0.050623   \n",
       "12   -0.078333 -0.039400 -0.035478 -0.015266 -0.015293 -0.003589  0.003317   \n",
       "13   -0.013076  0.015101  0.027279  0.005082  0.014445  0.000578  0.012420   \n",
       "14    0.002029 -0.006340 -0.004841 -0.004535 -0.001540 -0.002548 -0.002430   \n",
       "15   -0.128042  0.006306  0.063154  0.049963  0.035655  0.024088  0.000072   \n",
       "16    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "17   -0.013313 -0.020226 -0.024988 -0.017642  0.075197 -0.016114 -0.011271   \n",
       "18   -0.020948 -0.018588 -0.026161 -0.005675 -0.001587 -0.002780 -0.003908   \n",
       "19    0.013797 -0.036495 -0.014289  0.018623 -0.017866 -0.006413  0.006277   \n",
       "20    0.015284  0.012368  0.024529  0.023183  0.010369  0.011631  0.012967   \n",
       "21   -0.004894 -0.002317  0.000647  0.001272  0.003164  0.004028  0.006231   \n",
       "22    0.003639  0.003092  0.002663 -0.002966 -0.001633 -0.000295 -0.002939   \n",
       "23    0.000051  0.001603  0.003666  0.001969  0.001370  0.002672 -0.025949   \n",
       "24    0.001468 -0.007685 -0.002316  0.009876  0.001603 -0.000779 -0.002800   \n",
       "25   -0.079307 -0.044940 -0.004161 -0.000975 -0.000746 -0.002121  0.040337   \n",
       "26    0.044073  0.037817  0.023229  0.023214  0.052923  0.068834  0.099976   \n",
       "27   -0.002332 -0.014680 -0.030029 -0.045095 -0.024173 -0.009167 -0.000573   \n",
       "28   -0.005987  0.002439  0.006310 -0.000734  0.002256  0.009119  0.009968   \n",
       "29   -0.000655 -0.001665  0.051067  0.020941  0.043685  0.081384  0.023919   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1970 -0.049367 -0.023214 -0.012704 -0.010242  0.000182  0.023740  0.023142   \n",
       "1971  0.004169  0.004311  0.000280  0.024825  0.001135 -0.103117 -0.138674   \n",
       "1972 -0.019942 -0.023973 -0.021969 -0.012402  0.004200  0.002875 -0.009791   \n",
       "1973  0.029817  0.028034 -0.001200 -0.012980 -0.035495 -0.089925 -0.080743   \n",
       "1974 -0.000207  0.001634  0.002909  0.002384  0.008625  0.023292  0.019564   \n",
       "1975 -0.000010  0.000599  0.000710  0.001807  0.003740  0.008266  0.012714   \n",
       "1976  0.014453  0.006857  0.000321  0.008958  0.014514  0.009214  0.005750   \n",
       "1977 -0.066317 -0.029153  0.046721  0.053436  0.012910 -0.035238 -0.002804   \n",
       "1978 -0.000708 -0.031648 -0.028026 -0.010788 -0.023476 -0.009266 -0.014904   \n",
       "1979  0.010402  0.004792  0.000058 -0.004293 -0.006872 -0.004842 -0.001236   \n",
       "1980 -0.000063 -0.000736 -0.001383 -0.000466 -0.005586  0.006051  0.001748   \n",
       "1981 -0.014033  0.018120 -0.004490 -0.028622  0.018973  0.006707 -0.035800   \n",
       "1982  0.014439  0.013683  0.009910  0.010641  0.006615  0.001565  0.001791   \n",
       "1983  0.007101  0.000566  0.001267  0.000041  0.000036  0.002923  0.003814   \n",
       "1984 -0.007241 -0.007990 -0.024073 -0.017647 -0.012083 -0.005893 -0.002557   \n",
       "1985 -0.043725 -0.020229  0.002720 -0.021290 -0.050453 -0.048004 -0.004628   \n",
       "1986  0.016831  0.020305  0.010044  0.029877  0.055909  0.073108  0.078796   \n",
       "1987  0.111790  0.084298  0.091169  0.074928  0.087373  0.024236 -0.190039   \n",
       "1988 -0.003241  0.004197  0.004403  0.004030  0.003849  0.003754  0.003653   \n",
       "1989  0.009668 -0.004713  0.003391  0.001080 -0.054880 -0.064890 -0.025821   \n",
       "1990 -0.000440 -0.040486 -0.004746  0.019339 -0.011171 -0.023570  0.005669   \n",
       "1991 -0.006729  0.011705  0.047846  0.014878  0.005887  0.000025  0.010107   \n",
       "1992 -0.097138 -0.079585 -0.059086 -0.052043 -0.034967 -0.019995  0.004845   \n",
       "1993 -0.093245 -0.058551  0.050890  0.020120 -0.005391 -0.011347 -0.006256   \n",
       "1994 -0.147514 -0.066310  0.012779  0.001525  0.001099  0.003321  0.004485   \n",
       "1995 -0.006021 -0.004997 -0.003527 -0.004530 -0.008682 -0.013278 -0.010563   \n",
       "1996 -0.033343 -0.022161 -0.012821 -0.020406 -0.020390 -0.013854 -0.017466   \n",
       "1997  0.022203  0.035480  0.007251 -0.050066 -0.012828  0.041415  0.043532   \n",
       "1998  0.030341  0.014014  0.023996 -0.046175  0.005900  0.036697 -0.069256   \n",
       "1999 -0.022427 -0.010502  0.006985 -0.004213 -0.007820 -0.001414  0.004402   \n",
       "\n",
       "          7         8         9     ...       3063      3064      3065  \\\n",
       "0     0.015677 -0.049118 -0.051430  ...   0.013725  0.123213 -0.006638   \n",
       "1     0.078035  0.058132  0.008797  ...  -0.010371 -0.016821  0.008783   \n",
       "2    -0.028056 -0.005516 -0.025364  ...   0.000917  0.008904  0.010832   \n",
       "3     0.093836  0.089580  0.089868  ...   0.003782  0.015810  0.006891   \n",
       "4    -0.040445 -0.028034  0.059742  ...   0.004841  0.013999  0.020970   \n",
       "5    -0.000483 -0.000515 -0.000552  ...   0.000184  0.002762 -0.004692   \n",
       "6    -0.033114 -0.031494 -0.038910  ...   0.006623 -0.002786 -0.002075   \n",
       "7     0.003266 -0.004742 -0.006500  ...  -0.005236 -0.001792 -0.003260   \n",
       "8    -0.032693 -0.025744  0.009961  ...  -0.003864 -0.002841 -0.015710   \n",
       "9     0.017393  0.029579  0.020493  ...   0.032927  0.020618 -0.002640   \n",
       "10    0.068455  0.077711  0.066538  ...   0.009768  0.006889  0.005848   \n",
       "11    0.051277  0.033961  0.055225  ...   0.003214 -0.009795  0.005100   \n",
       "12   -0.008636 -0.035172 -0.028796  ...   0.011291  0.004020  0.008909   \n",
       "13    0.024485 -0.015884 -0.034665  ...  -0.005504 -0.013007 -0.046126   \n",
       "14   -0.002305 -0.002202 -0.002327  ...   0.000097  0.002047 -0.001220   \n",
       "15   -0.005964 -0.028265 -0.069240  ...  -0.106236  0.043024  0.026064   \n",
       "16    0.000000  0.000000 -0.000001  ...   0.043850  0.035850  0.025108   \n",
       "17    0.006715 -0.002664  0.003054  ...   0.016604  0.016532  0.000467   \n",
       "18   -0.003197 -0.003252 -0.005406  ...   0.010153 -0.001188 -0.009929   \n",
       "19    0.006650 -0.007309 -0.007378  ...  -0.019849 -0.029290 -0.042688   \n",
       "20    0.006176  0.004024  0.001508  ...  -0.043750 -0.002809 -0.013538   \n",
       "21    0.004969  0.002798  0.001769  ...  -0.008992 -0.012087  0.006003   \n",
       "22    0.003335 -0.000117 -0.028369  ...   0.009058 -0.020949 -0.022272   \n",
       "23   -0.010128  0.192425  0.056936  ...  -0.002019  0.001963  0.021156   \n",
       "24   -0.002749  0.004483 -0.000274  ...   0.007260  0.010406  0.002906   \n",
       "25   -0.035029 -0.088008  0.022112  ...   0.010164 -0.009338  0.004636   \n",
       "26    0.119910  0.124708  0.129756  ...   0.100456  0.080219  0.058940   \n",
       "27   -0.011225 -0.008913 -0.008394  ...   0.001986 -0.001504  0.003195   \n",
       "28    0.006015  0.005826  0.003416  ...  -0.008532  0.003724  0.025260   \n",
       "29   -0.008496 -0.062231  0.004672  ...  -0.019435 -0.043152 -0.053377   \n",
       "...        ...       ...       ...  ...        ...       ...       ...   \n",
       "1970 -0.096094 -0.009804  0.089443  ...  -0.017684  0.027654  0.070135   \n",
       "1971  0.015781  0.016727  0.002762  ...   0.004385  0.004299 -0.000568   \n",
       "1972 -0.029427 -0.042189 -0.018761  ...   0.011086  0.012365 -0.017993   \n",
       "1973 -0.072421 -0.047086 -0.040740  ...   0.003222 -0.012463 -0.003267   \n",
       "1974  0.015520  0.004113 -0.013274  ...   0.001673  0.000690  0.000121   \n",
       "1975  0.016753  0.025388  0.027456  ...   0.003038  0.001600  0.001108   \n",
       "1976  0.002227  0.006196 -0.004517  ...   0.020406  0.028140  0.016670   \n",
       "1977  0.032899  0.074104  0.076151  ...  -0.014913  0.000404 -0.001300   \n",
       "1978 -0.047553 -0.022767  0.011040  ...  -0.031812 -0.005208  0.026227   \n",
       "1979 -0.037958 -0.043809 -0.028412  ...  -0.012415 -0.020330 -0.020171   \n",
       "1980 -0.013153  0.004683 -0.000745  ...  -0.002964 -0.039568 -0.032793   \n",
       "1981 -0.009398  0.018722 -0.004503  ...   0.031860  0.060749  0.026839   \n",
       "1982 -0.021253 -0.037579 -0.060752  ...  -0.033646 -0.023532 -0.019896   \n",
       "1983  0.002789  0.002874  0.002749  ...  -0.002502  0.000065 -0.000098   \n",
       "1984 -0.004222 -0.008775 -0.012841  ...   0.022167  0.021740  0.015257   \n",
       "1985  0.051912  0.086225  0.114307  ...  -0.058727 -0.055159 -0.062923   \n",
       "1986  0.084910  0.069699  0.016255  ...  -0.014804  0.002547  0.010930   \n",
       "1987  0.012912 -0.002945 -0.066698  ...  -0.032753  0.027317  0.010146   \n",
       "1988  0.005744 -0.000257  0.003945  ...   0.052756  0.049592  0.048908   \n",
       "1989 -0.066461 -0.104204 -0.053223  ...   0.007417 -0.021339 -0.011602   \n",
       "1990  0.001446  0.000790 -0.001433  ...   0.047149  0.032900  0.019979   \n",
       "1991 -0.002872 -0.001249  0.021277  ...   0.067558  0.082918  0.072749   \n",
       "1992  0.012336  0.014885  0.018788  ...   0.010546  0.001651  0.006047   \n",
       "1993  0.017615  0.030588  0.031824  ...   0.015452  0.011464  0.006536   \n",
       "1994 -0.001618  0.007575  0.035502  ...   0.009267  0.010739  0.015231   \n",
       "1995 -0.009648 -0.008027  0.003786  ...  -0.044083 -0.041418 -0.042381   \n",
       "1996 -0.014241 -0.016238 -0.016527  ...   0.000734 -0.004880 -0.005046   \n",
       "1997  0.033096  0.042523 -0.014916  ...  -0.006024 -0.013834  0.009403   \n",
       "1998 -0.063223  0.025327  0.015869  ...  -0.023376 -0.008562  0.043392   \n",
       "1999  0.002292  0.004221  0.006314  ...   0.040203  0.033803  0.029467   \n",
       "\n",
       "          3066      3067      3068      3069      3070      3071  3072  \n",
       "0    -0.043762 -0.011082 -0.003506 -0.007296 -0.020889 -0.024223   NaN  \n",
       "1    -0.007583 -0.024359 -0.002662 -0.003553 -0.000605  0.000030   NaN  \n",
       "2    -0.000609  0.001183  0.000750 -0.006280  0.018178  0.009699   NaN  \n",
       "3    -0.001391  0.005947  0.015124  0.012474 -0.013197 -0.037887   NaN  \n",
       "4     0.043955  0.026964  0.030705 -0.002225  0.009616  0.006040   NaN  \n",
       "5    -0.004202 -0.003634 -0.000311 -0.004221 -0.003897  0.002847   NaN  \n",
       "6    -0.004846 -0.002113 -0.006232 -0.014587 -0.076449 -0.031167   NaN  \n",
       "7    -0.003570 -0.001974  0.008249 -0.001909 -0.002572 -0.002357   NaN  \n",
       "8    -0.013947  0.006856  0.030468  0.056211  0.056140  0.060534   NaN  \n",
       "9     0.008354 -0.005770  0.008310 -0.005025 -0.000503 -0.004402   NaN  \n",
       "10    0.009870  0.010684  0.018351  0.018319  0.007916 -0.010940   NaN  \n",
       "11    0.003709  0.016094  0.000127  0.005235  0.036845 -0.009058   NaN  \n",
       "12    0.009295 -0.003402 -0.005555  0.009537  0.016247  0.019649   NaN  \n",
       "13   -0.012297  0.042970  0.033769  0.005311 -0.080462  0.036623   NaN  \n",
       "14    0.000749  0.000260 -0.000020  0.000079  0.000468  0.000440   NaN  \n",
       "15   -0.070579 -0.080341 -0.062883 -0.011459  0.013694  0.003556   NaN  \n",
       "16    0.019560  0.022995 -0.000538  0.017666  0.027076  0.009692   NaN  \n",
       "17   -0.010608 -0.015623 -0.045108 -0.082262 -0.099732 -0.011712   NaN  \n",
       "18   -0.012815 -0.001753  0.004605  0.001046  0.003289  0.004990   NaN  \n",
       "19    0.001996  0.045124  0.032707 -0.004925  0.040127  0.009098   NaN  \n",
       "20   -0.012697 -0.010171 -0.009076 -0.008570 -0.002556 -0.001947   NaN  \n",
       "21    0.026791 -0.003236 -0.005615  0.008982 -0.014955 -0.012604   NaN  \n",
       "22    0.010033  0.007964  0.006688 -0.024299 -0.015483  0.018256   NaN  \n",
       "23    0.013631 -0.027612 -0.024798  0.008288  0.008967 -0.026417   NaN  \n",
       "24    0.020847  0.012062  0.003060  0.012369  0.010780 -0.002764   NaN  \n",
       "25    0.005752 -0.010149 -0.021101 -0.006629  0.012045  0.041208   NaN  \n",
       "26    0.037133  0.039924  0.045988  0.035910  0.013805  0.035574   NaN  \n",
       "27    0.004896 -0.001716 -0.003208 -0.003658  0.001874  0.001529   NaN  \n",
       "28    0.005692 -0.004188 -0.016492 -0.008390 -0.009993  0.006029   NaN  \n",
       "29   -0.043291 -0.042670 -0.008868  0.013367  0.011265  0.004730   NaN  \n",
       "...        ...       ...       ...       ...       ...       ...   ...  \n",
       "1970  0.047392  0.068083  0.104659  0.005392 -0.015622 -0.067773   NaN  \n",
       "1971  0.001926 -0.002688  0.000741  0.000268 -0.000509 -0.000291   NaN  \n",
       "1972 -0.001214 -0.013841  0.018112 -0.006222 -0.004652  0.026367   NaN  \n",
       "1973  0.013649  0.024968  0.044639 -0.006488 -0.042983 -0.001836   NaN  \n",
       "1974 -0.000064  0.000079  0.000112  0.001722  0.001723 -0.005875   NaN  \n",
       "1975  0.000733  0.000687  0.000645  0.000628  0.000573  0.000565   NaN  \n",
       "1976  0.012407  0.022417  0.007028 -0.000088  0.018758 -0.005744   NaN  \n",
       "1977  0.001271 -0.027878  0.016780  0.038456 -0.005518  0.008696   NaN  \n",
       "1978  0.008688 -0.075301  0.012872  0.007400 -0.054902 -0.085861   NaN  \n",
       "1979 -0.011733 -0.009458 -0.003552 -0.007833 -0.007810 -0.031812   NaN  \n",
       "1980 -0.004230  0.028499  0.024672  0.031623  0.031356  0.030750   NaN  \n",
       "1981 -0.001880 -0.005826 -0.006457  0.012299  0.010584 -0.001859   NaN  \n",
       "1982 -0.022324 -0.012913 -0.026536 -0.033833 -0.024506 -0.024253   NaN  \n",
       "1983 -0.001410  0.001013  0.000798  0.001095  0.001282 -0.000330   NaN  \n",
       "1984 -0.046881 -0.065418 -0.014639  0.022241 -0.039683 -0.053298   NaN  \n",
       "1985 -0.064047 -0.068489 -0.069911 -0.071826 -0.069520 -0.074040   NaN  \n",
       "1986 -0.007617 -0.024243 -0.002141  0.002854  0.018120  0.047070   NaN  \n",
       "1987  0.004048  0.014203 -0.017538  0.012408  0.057850  0.028633   NaN  \n",
       "1988  0.051547  0.047621  0.041506  0.044924  0.040239  0.010878   NaN  \n",
       "1989  0.004651 -0.001503 -0.009852 -0.031243  0.045586  0.046126   NaN  \n",
       "1990  0.010586 -0.003565 -0.006615 -0.004334 -0.002753 -0.004215   NaN  \n",
       "1991  0.048206  0.044773  0.065448  0.060457  0.008385 -0.001930   NaN  \n",
       "1992  0.011601  0.011044 -0.006630 -0.006376  0.003645  0.006604   NaN  \n",
       "1993  0.010482  0.006423  0.001348 -0.001634  0.004288 -0.003357   NaN  \n",
       "1994  0.011451 -0.000737  0.007873 -0.005385 -0.036363  0.028626   NaN  \n",
       "1995 -0.063451 -0.012183 -0.045378 -0.076428  0.024682 -0.044529   NaN  \n",
       "1996 -0.009637 -0.009953 -0.014024 -0.011671 -0.009456 -0.014382   NaN  \n",
       "1997 -0.005050 -0.018284  0.008724  0.008318  0.000721 -0.002122   NaN  \n",
       "1998  0.043874  0.007881  0.035408  0.015650  0.042028 -0.002508   NaN  \n",
       "1999  0.008148 -0.001093 -0.000264  0.000986  0.001235 -0.009715   NaN  \n",
       "\n",
       "[2000 rows x 3073 columns]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df_train_cleared = df_train.drop(3072, 1)\n",
    "df_test_cleared = df_test.drop(3072, 1)\n",
    "y_train = np.array(df_train_target[1])\n",
    "X_train = np.array(df_train_cleared).reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)# + np.array([0.25, 0.2, 0.2])\n",
    "X_test = np.array(df_test_cleared).reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nvSQVzqCXpMIZ9JJUuP8H10DCZycyqekAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f950193a850>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist((X_train + np.array([0.25, 0.2, 0.2])).ravel())\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 of 1000 took 0.308s\n",
      "  training loss (in-iteration):\t\t2.303082\n",
      "  train accuracy:\t\t9.40 %\n",
      "  validation accuracy:\t\t10.60 %\n",
      "Epoch 10 of 1000 took 0.269s\n",
      "  training loss (in-iteration):\t\t2.148508\n",
      "  train accuracy:\t\t20.40 %\n",
      "  validation accuracy:\t\t19.30 %\n",
      "Epoch 20 of 1000 took 0.270s\n",
      "  training loss (in-iteration):\t\t2.003328\n",
      "  train accuracy:\t\t26.50 %\n",
      "  validation accuracy:\t\t24.70 %\n",
      "Epoch 30 of 1000 took 0.271s\n",
      "  training loss (in-iteration):\t\t1.914612\n",
      "  train accuracy:\t\t29.40 %\n",
      "  validation accuracy:\t\t25.50 %\n",
      "Epoch 40 of 1000 took 0.272s\n",
      "  training loss (in-iteration):\t\t1.854587\n",
      "  train accuracy:\t\t32.10 %\n",
      "  validation accuracy:\t\t27.50 %\n",
      "Epoch 50 of 1000 took 0.277s\n",
      "  training loss (in-iteration):\t\t1.806063\n",
      "  train accuracy:\t\t33.40 %\n",
      "  validation accuracy:\t\t28.50 %\n",
      "Epoch 60 of 1000 took 0.276s\n",
      "  training loss (in-iteration):\t\t1.762832\n",
      "  train accuracy:\t\t35.27 %\n",
      "  validation accuracy:\t\t29.00 %\n",
      "Epoch 70 of 1000 took 0.277s\n",
      "  training loss (in-iteration):\t\t1.727092\n",
      "  train accuracy:\t\t37.37 %\n",
      "  validation accuracy:\t\t29.30 %\n",
      "Epoch 80 of 1000 took 0.275s\n",
      "  training loss (in-iteration):\t\t1.687452\n",
      "  train accuracy:\t\t38.67 %\n",
      "  validation accuracy:\t\t28.80 %\n",
      "Epoch 90 of 1000 took 0.277s\n",
      "  training loss (in-iteration):\t\t1.662172\n",
      "  train accuracy:\t\t40.07 %\n",
      "  validation accuracy:\t\t29.70 %\n",
      "Epoch 100 of 1000 took 0.277s\n",
      "  training loss (in-iteration):\t\t1.628791\n",
      "  train accuracy:\t\t42.03 %\n",
      "  validation accuracy:\t\t31.10 %\n",
      "Epoch 110 of 1000 took 0.278s\n",
      "  training loss (in-iteration):\t\t1.602869\n",
      "  train accuracy:\t\t41.77 %\n",
      "  validation accuracy:\t\t30.90 %\n",
      "Epoch 120 of 1000 took 0.277s\n",
      "  training loss (in-iteration):\t\t1.545525\n",
      "  train accuracy:\t\t44.17 %\n",
      "  validation accuracy:\t\t31.20 %\n",
      "Epoch 130 of 1000 took 0.280s\n",
      "  training loss (in-iteration):\t\t1.544602\n",
      "  train accuracy:\t\t46.60 %\n",
      "  validation accuracy:\t\t32.40 %\n",
      "Epoch 140 of 1000 took 0.284s\n",
      "  training loss (in-iteration):\t\t1.463642\n",
      "  train accuracy:\t\t47.70 %\n",
      "  validation accuracy:\t\t32.20 %\n",
      "Epoch 150 of 1000 took 0.277s\n",
      "  training loss (in-iteration):\t\t1.453317\n",
      "  train accuracy:\t\t48.40 %\n",
      "  validation accuracy:\t\t33.20 %\n",
      "Epoch 160 of 1000 took 0.278s\n",
      "  training loss (in-iteration):\t\t1.399344\n",
      "  train accuracy:\t\t50.57 %\n",
      "  validation accuracy:\t\t32.20 %\n",
      "Epoch 170 of 1000 took 0.283s\n",
      "  training loss (in-iteration):\t\t1.403646\n",
      "  train accuracy:\t\t51.70 %\n",
      "  validation accuracy:\t\t30.50 %\n",
      "Epoch 180 of 1000 took 0.281s\n",
      "  training loss (in-iteration):\t\t1.346752\n",
      "  train accuracy:\t\t54.07 %\n",
      "  validation accuracy:\t\t33.10 %\n",
      "Epoch 190 of 1000 took 0.284s\n",
      "  training loss (in-iteration):\t\t1.333620\n",
      "  train accuracy:\t\t53.17 %\n",
      "  validation accuracy:\t\t33.50 %\n",
      "Epoch 200 of 1000 took 0.278s\n",
      "  training loss (in-iteration):\t\t1.257822\n",
      "  train accuracy:\t\t56.20 %\n",
      "  validation accuracy:\t\t35.30 %\n",
      "Epoch 210 of 1000 took 0.284s\n",
      "  training loss (in-iteration):\t\t1.214607\n",
      "  train accuracy:\t\t57.73 %\n",
      "  validation accuracy:\t\t33.80 %\n",
      "Epoch 220 of 1000 took 0.284s\n",
      "  training loss (in-iteration):\t\t1.213842\n",
      "  train accuracy:\t\t58.23 %\n",
      "  validation accuracy:\t\t33.50 %\n",
      "Epoch 230 of 1000 took 0.284s\n",
      "  training loss (in-iteration):\t\t1.172612\n",
      "  train accuracy:\t\t59.93 %\n",
      "  validation accuracy:\t\t32.80 %\n",
      "Epoch 240 of 1000 took 0.280s\n",
      "  training loss (in-iteration):\t\t1.152752\n",
      "  train accuracy:\t\t59.53 %\n",
      "  validation accuracy:\t\t33.70 %\n",
      "Epoch 250 of 1000 took 0.280s\n",
      "  training loss (in-iteration):\t\t1.105005\n",
      "  train accuracy:\t\t61.97 %\n",
      "  validation accuracy:\t\t32.10 %\n",
      "Epoch 260 of 1000 took 0.279s\n",
      "  training loss (in-iteration):\t\t1.079235\n",
      "  train accuracy:\t\t63.03 %\n",
      "  validation accuracy:\t\t32.10 %\n",
      "Epoch 270 of 1000 took 0.279s\n",
      "  training loss (in-iteration):\t\t1.014126\n",
      "  train accuracy:\t\t65.93 %\n",
      "  validation accuracy:\t\t33.90 %\n",
      "Epoch 280 of 1000 took 0.284s\n",
      "  training loss (in-iteration):\t\t1.022513\n",
      "  train accuracy:\t\t65.03 %\n",
      "  validation accuracy:\t\t33.30 %\n",
      "Epoch 290 of 1000 took 0.285s\n",
      "  training loss (in-iteration):\t\t0.974859\n",
      "  train accuracy:\t\t67.00 %\n",
      "  validation accuracy:\t\t33.10 %\n",
      "Epoch 300 of 1000 took 0.290s\n",
      "  training loss (in-iteration):\t\t0.912405\n",
      "  train accuracy:\t\t69.90 %\n",
      "  validation accuracy:\t\t34.90 %\n",
      "Epoch 310 of 1000 took 0.289s\n",
      "  training loss (in-iteration):\t\t0.901842\n",
      "  train accuracy:\t\t70.90 %\n",
      "  validation accuracy:\t\t33.40 %\n",
      "Epoch 320 of 1000 took 0.289s\n",
      "  training loss (in-iteration):\t\t0.866982\n",
      "  train accuracy:\t\t71.57 %\n",
      "  validation accuracy:\t\t33.80 %\n",
      "Epoch 330 of 1000 took 0.287s\n",
      "  training loss (in-iteration):\t\t0.836516\n",
      "  train accuracy:\t\t73.33 %\n",
      "  validation accuracy:\t\t33.40 %\n",
      "Epoch 340 of 1000 took 0.286s\n",
      "  training loss (in-iteration):\t\t0.844216\n",
      "  train accuracy:\t\t71.53 %\n",
      "  validation accuracy:\t\t33.70 %\n",
      "Epoch 350 of 1000 took 0.288s\n",
      "  training loss (in-iteration):\t\t0.788018\n",
      "  train accuracy:\t\t74.10 %\n",
      "  validation accuracy:\t\t32.70 %\n",
      "Epoch 360 of 1000 took 0.284s\n",
      "  training loss (in-iteration):\t\t0.779257\n",
      "  train accuracy:\t\t74.13 %\n",
      "  validation accuracy:\t\t34.60 %\n",
      "Epoch 370 of 1000 took 0.290s\n",
      "  training loss (in-iteration):\t\t0.753324\n",
      "  train accuracy:\t\t74.93 %\n",
      "  validation accuracy:\t\t33.10 %\n",
      "Epoch 380 of 1000 took 0.287s\n",
      "  training loss (in-iteration):\t\t0.696357\n",
      "  train accuracy:\t\t78.43 %\n",
      "  validation accuracy:\t\t33.70 %\n",
      "Epoch 390 of 1000 took 0.287s\n",
      "  training loss (in-iteration):\t\t0.629316\n",
      "  train accuracy:\t\t82.20 %\n",
      "  validation accuracy:\t\t32.90 %\n",
      "Epoch 400 of 1000 took 0.287s\n",
      "  training loss (in-iteration):\t\t0.676505\n",
      "  train accuracy:\t\t79.53 %\n",
      "  validation accuracy:\t\t32.80 %\n",
      "Epoch 410 of 1000 took 0.286s\n",
      "  training loss (in-iteration):\t\t0.593475\n",
      "  train accuracy:\t\t82.83 %\n",
      "  validation accuracy:\t\t33.60 %\n",
      "Epoch 420 of 1000 took 0.287s\n",
      "  training loss (in-iteration):\t\t0.575645\n",
      "  train accuracy:\t\t83.10 %\n",
      "  validation accuracy:\t\t32.50 %\n",
      "Epoch 430 of 1000 took 0.287s\n",
      "  training loss (in-iteration):\t\t0.566923\n",
      "  train accuracy:\t\t83.93 %\n",
      "  validation accuracy:\t\t34.10 %\n",
      "Epoch 440 of 1000 took 0.287s\n",
      "  training loss (in-iteration):\t\t0.509675\n",
      "  train accuracy:\t\t87.00 %\n",
      "  validation accuracy:\t\t33.70 %\n",
      "Epoch 450 of 1000 took 0.287s\n",
      "  training loss (in-iteration):\t\t0.475332\n",
      "  train accuracy:\t\t88.17 %\n",
      "  validation accuracy:\t\t34.40 %\n",
      "Epoch 460 of 1000 took 0.287s\n",
      "  training loss (in-iteration):\t\t0.444435\n",
      "  train accuracy:\t\t89.43 %\n",
      "  validation accuracy:\t\t33.50 %\n",
      "Epoch 470 of 1000 took 0.289s\n",
      "  training loss (in-iteration):\t\t0.457246\n",
      "  train accuracy:\t\t88.37 %\n",
      "  validation accuracy:\t\t32.40 %\n",
      "Epoch 480 of 1000 took 0.289s\n",
      "  training loss (in-iteration):\t\t0.384736\n",
      "  train accuracy:\t\t91.83 %\n",
      "  validation accuracy:\t\t32.90 %\n",
      "Epoch 490 of 1000 took 0.289s\n",
      "  training loss (in-iteration):\t\t0.371189\n",
      "  train accuracy:\t\t92.03 %\n",
      "  validation accuracy:\t\t32.90 %\n",
      "Epoch 500 of 1000 took 0.287s\n",
      "  training loss (in-iteration):\t\t0.358931\n",
      "  train accuracy:\t\t92.73 %\n",
      "  validation accuracy:\t\t33.20 %\n",
      "Epoch 510 of 1000 took 0.287s\n",
      "  training loss (in-iteration):\t\t0.356328\n",
      "  train accuracy:\t\t92.47 %\n",
      "  validation accuracy:\t\t31.40 %\n",
      "Epoch 520 of 1000 took 0.287s\n",
      "  training loss (in-iteration):\t\t0.292670\n",
      "  train accuracy:\t\t95.13 %\n",
      "  validation accuracy:\t\t33.20 %\n",
      "Epoch 530 of 1000 took 0.287s\n",
      "  training loss (in-iteration):\t\t0.278993\n",
      "  train accuracy:\t\t95.53 %\n",
      "  validation accuracy:\t\t32.20 %\n",
      "Epoch 540 of 1000 took 0.287s\n",
      "  training loss (in-iteration):\t\t0.309015\n",
      "  train accuracy:\t\t93.53 %\n",
      "  validation accuracy:\t\t32.70 %\n",
      "Epoch 550 of 1000 took 0.287s\n",
      "  training loss (in-iteration):\t\t0.244430\n",
      "  train accuracy:\t\t96.33 %\n",
      "  validation accuracy:\t\t33.40 %\n",
      "Epoch 560 of 1000 took 0.287s\n",
      "  training loss (in-iteration):\t\t0.219642\n",
      "  train accuracy:\t\t97.27 %\n",
      "  validation accuracy:\t\t32.90 %\n",
      "Epoch 570 of 1000 took 0.287s\n",
      "  training loss (in-iteration):\t\t0.201234\n",
      "  train accuracy:\t\t97.73 %\n",
      "  validation accuracy:\t\t32.30 %\n",
      "Epoch 580 of 1000 took 0.287s\n",
      "  training loss (in-iteration):\t\t0.197938\n",
      "  train accuracy:\t\t97.57 %\n",
      "  validation accuracy:\t\t32.10 %\n",
      "Epoch 590 of 1000 took 0.287s\n",
      "  training loss (in-iteration):\t\t0.169131\n",
      "  train accuracy:\t\t98.53 %\n",
      "  validation accuracy:\t\t32.20 %\n",
      "Epoch 600 of 1000 took 0.287s\n",
      "  training loss (in-iteration):\t\t0.174982\n",
      "  train accuracy:\t\t98.10 %\n",
      "  validation accuracy:\t\t32.30 %\n",
      "Epoch 610 of 1000 took 0.287s\n",
      "  training loss (in-iteration):\t\t0.156690\n",
      "  train accuracy:\t\t98.67 %\n",
      "  validation accuracy:\t\t32.20 %\n",
      "Epoch 620 of 1000 took 0.288s\n",
      "  training loss (in-iteration):\t\t0.133538\n",
      "  train accuracy:\t\t99.40 %\n",
      "  validation accuracy:\t\t32.60 %\n",
      "Epoch 630 of 1000 took 0.284s\n",
      "  training loss (in-iteration):\t\t0.116220\n",
      "  train accuracy:\t\t99.53 %\n",
      "  validation accuracy:\t\t32.70 %\n",
      "Epoch 640 of 1000 took 0.287s\n",
      "  training loss (in-iteration):\t\t0.112750\n",
      "  train accuracy:\t\t99.60 %\n",
      "  validation accuracy:\t\t31.70 %\n",
      "Epoch 650 of 1000 took 0.287s\n",
      "  training loss (in-iteration):\t\t0.114000\n",
      "  train accuracy:\t\t99.43 %\n",
      "  validation accuracy:\t\t32.50 %\n",
      "Epoch 660 of 1000 took 0.288s\n",
      "  training loss (in-iteration):\t\t0.091116\n",
      "  train accuracy:\t\t99.73 %\n",
      "  validation accuracy:\t\t31.90 %\n",
      "Epoch 670 of 1000 took 0.287s\n",
      "  training loss (in-iteration):\t\t0.082695\n",
      "  train accuracy:\t\t99.83 %\n",
      "  validation accuracy:\t\t31.90 %\n",
      "Epoch 680 of 1000 took 0.287s\n",
      "  training loss (in-iteration):\t\t0.078171\n",
      "  train accuracy:\t\t99.77 %\n",
      "  validation accuracy:\t\t32.20 %\n",
      "Epoch 690 of 1000 took 0.285s\n",
      "  training loss (in-iteration):\t\t0.074287\n",
      "  train accuracy:\t\t99.87 %\n",
      "  validation accuracy:\t\t32.60 %\n",
      "Epoch 700 of 1000 took 0.285s\n",
      "  training loss (in-iteration):\t\t0.065233\n",
      "  train accuracy:\t\t99.90 %\n",
      "  validation accuracy:\t\t32.50 %\n",
      "Epoch 710 of 1000 took 0.287s\n",
      "  training loss (in-iteration):\t\t0.058363\n",
      "  train accuracy:\t\t99.90 %\n",
      "  validation accuracy:\t\t31.30 %\n",
      "Epoch 720 of 1000 took 0.288s\n",
      "  training loss (in-iteration):\t\t0.050941\n",
      "  train accuracy:\t\t99.97 %\n",
      "  validation accuracy:\t\t32.30 %\n",
      "Epoch 730 of 1000 took 0.285s\n",
      "  training loss (in-iteration):\t\t0.051030\n",
      "  train accuracy:\t\t99.97 %\n",
      "  validation accuracy:\t\t32.30 %\n",
      "Epoch 740 of 1000 took 0.285s\n",
      "  training loss (in-iteration):\t\t0.044557\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t31.40 %\n",
      "Epoch 750 of 1000 took 0.285s\n",
      "  training loss (in-iteration):\t\t0.041038\n",
      "  train accuracy:\t\t99.97 %\n",
      "  validation accuracy:\t\t32.10 %\n",
      "Epoch 760 of 1000 took 0.283s\n",
      "  training loss (in-iteration):\t\t0.040003\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t31.30 %\n",
      "Epoch 770 of 1000 took 0.287s\n",
      "  training loss (in-iteration):\t\t0.034130\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t32.90 %\n",
      "Epoch 780 of 1000 took 0.287s\n",
      "  training loss (in-iteration):\t\t0.030819\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t30.90 %\n",
      "Epoch 790 of 1000 took 0.287s\n",
      "  training loss (in-iteration):\t\t0.029185\n",
      "  train accuracy:\t\t100.00 %\n",
      "  validation accuracy:\t\t32.30 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-168-357ad87e2c30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-164-a0c594748dc3>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(dataset, num_epochs, batchsize)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterate_minibatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mtrain_err_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0mtrain_err\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_err_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mtrain_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_acc_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mborisya/opt/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset = train_test_split(X_train.reshape(-1, 3, 32, 32).astype(np.float32), y_train.astype(np.int32))\n",
    "learn(dataset, num_epochs=1000, batchsize=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evaluate(dataset[1], dataset[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savez('trained_weights', *lasagne.layers.get_all_param_values(nn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with np.load('trained_weights.npz') as f:\n",
    "            param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "            lasagne.layers.set_all_param_values(nn, param_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = train_test_split(X_train.reshape(-1, 3, 32, 32).astype(np.float32), y_train.astype(np.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 32, 32, 3)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = np.argmax(predict(X_test.reshape(-1, 3, 32, 32).astype(np.float32)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('answers.txt', 'wb') as f:\n",
    "    for i, a in enumerate(pred):\n",
    "        f.write('{},{}\\n'.format(i+1, a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# VGG-16, 16-layer model from the paper:\n",
    "# \"Very Deep Convolutional Networks for Large-Scale Image Recognition\"\n",
    "# Original source: https://gist.github.com/ksimonyan/211839e770f7b538e2d8\n",
    "# License: see http://www.robots.ox.ac.uk/~vgg/research/very_deep/\n",
    "\n",
    "# Download pretrained weights from:\n",
    "# https://s3.amazonaws.com/lasagne/recipes/pretrained/imagenet/vgg16.pkl\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    net = {}\n",
    "    net['input'] = InputLayer((None, 3, 32, 32))\n",
    "    net['conv1_1'] = Conv2DLayer(\n",
    "        net['input'], 64, 3, pad=1, flip_filters=False)\n",
    "    net['conv1_2'] = Conv2DLayer(\n",
    "        net['conv1_1'], 64, 3, pad=1, flip_filters=False)\n",
    "    net['pool1'] = MaxPool2DLayer(net['conv1_2'], 2)\n",
    "    net['conv2_1'] = Conv2DLayer(\n",
    "        net['pool1'], 128, 3, pad=1, flip_filters=False)\n",
    "    net['conv2_2'] = Conv2DLayer(\n",
    "        net['conv2_1'], 128, 3, pad=1, flip_filters=False)\n",
    "    net['pool2'] = MaxPool2DLayer(net['conv2_2'], 2)\n",
    "    net['conv3_1'] = Conv2DLayer(\n",
    "        net['pool2'], 256, 3, pad=1, flip_filters=False)\n",
    "    net['conv3_2'] = Conv2DLayer(\n",
    "        net['conv3_1'], 256, 3, pad=1, flip_filters=False)\n",
    "    net['conv3_3'] = Conv2DLayer(\n",
    "        net['conv3_2'], 256, 3, pad=1, flip_filters=False)\n",
    "    net['pool3'] = MaxPool2DLayer(net['conv3_3'], 2)\n",
    "    net['conv4_1'] = Conv2DLayer(\n",
    "        net['pool3'], 512, 3, pad=1, flip_filters=False)\n",
    "    net['conv4_2'] = Conv2DLayer(\n",
    "        net['conv4_1'], 512, 3, pad=1, flip_filters=False)\n",
    "    net['conv4_3'] = Conv2DLayer(\n",
    "        net['conv4_2'], 512, 3, pad=1, flip_filters=False)\n",
    "    net['pool4'] = MaxPool2DLayer(net['conv4_3'], 2)\n",
    "    net['conv5_1'] = Conv2DLayer(\n",
    "        net['pool4'], 512, 3, pad=1, flip_filters=False)\n",
    "    net['conv5_2'] = Conv2DLayer(\n",
    "        net['conv5_1'], 512, 3, pad=1, flip_filters=False)\n",
    "    net['conv5_3'] = Conv2DLayer(\n",
    "        net['conv5_2'], 512, 3, pad=1, flip_filters=False)\n",
    "    net['pool5'] = MaxPool2DLayer(net['conv5_3'], 2)\n",
    "    net['fc6'] = DenseLayer(net['pool5'], num_units=4096)\n",
    "    net['fc6_dropout'] = DropoutLayer(net['fc6'], p=0.5)\n",
    "    net['fc7'] = DenseLayer(net['fc6_dropout'], num_units=4096)\n",
    "    net['fc7_dropout'] = DropoutLayer(net['fc7'], p=0.5)\n",
    "    net['fc8'] = DenseLayer(\n",
    "        net['fc7_dropout'], num_units=10, nonlinearity=None)\n",
    "    net['prob'] = NonlinearityLayer(net['fc8'], softmax)\n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vgg16 = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ! wget https://s3.amazonaws.com/lasagne/recipes/pretrained/imagenet/vgg16.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "        \n",
    "with open('vgg16.pkl') as f:\n",
    "    params = pickle.load(f)\n",
    "\n",
    "lasagne.layers.set_all_param_values(lasagne.layers.get_all_layers(vgg16['prob'])[:-6], params['param values'][:-6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = lasagne.layers.get_all_params(vgg16['prob'], trainable=True)[-6:]\n",
    "output = lasagne.layers.get_output(vgg16['prob'], input_X, deterministic=False)\n",
    "obj = lasagne.objectives.categorical_crossentropy(output, target).mean()\n",
    "updates = lasagne.updates.adagrad(obj, params, learning_rate=1e-3)\n",
    "accuracy = lasagne.objectives.categorical_accuracy(output, target).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = theano.function([input_X, target], [obj, accuracy], updates=updates)\n",
    "evaluate = theano.function([input_X, target], accuracy)\n",
    "predict = theano.function([input_X], lasagne.layers.get_output(nn, input_X, deterministic=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 of 1000 took 0.694s\n",
      "  training loss (in-iteration):\t\t2.179639\n",
      "  train accuracy:\t\t19.80 %\n",
      "  validation accuracy:\t\t20.30 %\n",
      "Epoch 10 of 1000 took 0.663s\n",
      "  training loss (in-iteration):\t\t1.963671\n",
      "  train accuracy:\t\t27.40 %\n",
      "  validation accuracy:\t\t26.20 %\n",
      "Epoch 20 of 1000 took 0.677s\n",
      "  training loss (in-iteration):\t\t1.960422\n",
      "  train accuracy:\t\t27.43 %\n",
      "  validation accuracy:\t\t29.00 %\n",
      "Epoch 30 of 1000 took 0.679s\n",
      "  training loss (in-iteration):\t\t1.943287\n",
      "  train accuracy:\t\t27.73 %\n",
      "  validation accuracy:\t\t26.50 %\n",
      "Epoch 40 of 1000 took 0.682s\n",
      "  training loss (in-iteration):\t\t1.938585\n",
      "  train accuracy:\t\t28.23 %\n",
      "  validation accuracy:\t\t26.40 %\n",
      "Epoch 50 of 1000 took 0.687s\n",
      "  training loss (in-iteration):\t\t1.945515\n",
      "  train accuracy:\t\t28.20 %\n",
      "  validation accuracy:\t\t27.60 %\n",
      "Epoch 60 of 1000 took 0.687s\n",
      "  training loss (in-iteration):\t\t1.939003\n",
      "  train accuracy:\t\t27.97 %\n",
      "  validation accuracy:\t\t27.10 %\n",
      "Epoch 70 of 1000 took 0.686s\n",
      "  training loss (in-iteration):\t\t1.942096\n",
      "  train accuracy:\t\t28.50 %\n",
      "  validation accuracy:\t\t26.90 %\n",
      "Epoch 80 of 1000 took 0.691s\n",
      "  training loss (in-iteration):\t\t1.930328\n",
      "  train accuracy:\t\t29.97 %\n",
      "  validation accuracy:\t\t27.70 %\n",
      "Epoch 90 of 1000 took 0.693s\n",
      "  training loss (in-iteration):\t\t1.928999\n",
      "  train accuracy:\t\t28.73 %\n",
      "  validation accuracy:\t\t27.50 %\n",
      "Epoch 100 of 1000 took 0.777s\n",
      "  training loss (in-iteration):\t\t1.925764\n",
      "  train accuracy:\t\t28.57 %\n",
      "  validation accuracy:\t\t27.00 %\n",
      "Epoch 110 of 1000 took 0.737s\n",
      "  training loss (in-iteration):\t\t1.928141\n",
      "  train accuracy:\t\t29.23 %\n",
      "  validation accuracy:\t\t27.20 %\n",
      "Epoch 120 of 1000 took 0.711s\n",
      "  training loss (in-iteration):\t\t1.914265\n",
      "  train accuracy:\t\t28.50 %\n",
      "  validation accuracy:\t\t28.20 %\n",
      "Epoch 130 of 1000 took 0.719s\n",
      "  training loss (in-iteration):\t\t1.925950\n",
      "  train accuracy:\t\t28.77 %\n",
      "  validation accuracy:\t\t29.10 %\n",
      "Epoch 140 of 1000 took 0.719s\n",
      "  training loss (in-iteration):\t\t1.916098\n",
      "  train accuracy:\t\t29.43 %\n",
      "  validation accuracy:\t\t27.10 %\n",
      "Epoch 150 of 1000 took 0.719s\n",
      "  training loss (in-iteration):\t\t1.906225\n",
      "  train accuracy:\t\t29.17 %\n",
      "  validation accuracy:\t\t28.70 %\n",
      "Epoch 160 of 1000 took 0.715s\n",
      "  training loss (in-iteration):\t\t1.911688\n",
      "  train accuracy:\t\t29.47 %\n",
      "  validation accuracy:\t\t28.30 %\n",
      "Epoch 170 of 1000 took 0.718s\n",
      "  training loss (in-iteration):\t\t1.911703\n",
      "  train accuracy:\t\t29.70 %\n",
      "  validation accuracy:\t\t28.40 %\n",
      "Epoch 180 of 1000 took 0.718s\n",
      "  training loss (in-iteration):\t\t1.894006\n",
      "  train accuracy:\t\t31.07 %\n",
      "  validation accuracy:\t\t28.00 %\n",
      "Epoch 190 of 1000 took 0.718s\n",
      "  training loss (in-iteration):\t\t1.891594\n",
      "  train accuracy:\t\t29.60 %\n",
      "  validation accuracy:\t\t30.00 %\n",
      "Epoch 200 of 1000 took 0.724s\n",
      "  training loss (in-iteration):\t\t1.904735\n",
      "  train accuracy:\t\t29.33 %\n",
      "  validation accuracy:\t\t27.00 %\n",
      "Epoch 210 of 1000 took 0.711s\n",
      "  training loss (in-iteration):\t\t1.901546\n",
      "  train accuracy:\t\t30.13 %\n",
      "  validation accuracy:\t\t28.10 %\n",
      "Epoch 220 of 1000 took 0.721s\n",
      "  training loss (in-iteration):\t\t1.893096\n",
      "  train accuracy:\t\t29.90 %\n",
      "  validation accuracy:\t\t27.70 %\n",
      "Epoch 230 of 1000 took 0.720s\n",
      "  training loss (in-iteration):\t\t1.899933\n",
      "  train accuracy:\t\t30.13 %\n",
      "  validation accuracy:\t\t27.20 %\n",
      "Epoch 240 of 1000 took 0.706s\n",
      "  training loss (in-iteration):\t\t1.903783\n",
      "  train accuracy:\t\t30.50 %\n",
      "  validation accuracy:\t\t28.90 %\n",
      "Epoch 250 of 1000 took 0.715s\n",
      "  training loss (in-iteration):\t\t1.895051\n",
      "  train accuracy:\t\t30.40 %\n",
      "  validation accuracy:\t\t27.70 %\n",
      "Epoch 260 of 1000 took 0.715s\n",
      "  training loss (in-iteration):\t\t1.893802\n",
      "  train accuracy:\t\t30.93 %\n",
      "  validation accuracy:\t\t27.80 %\n",
      "Epoch 270 of 1000 took 0.707s\n",
      "  training loss (in-iteration):\t\t1.892630\n",
      "  train accuracy:\t\t30.77 %\n",
      "  validation accuracy:\t\t26.80 %\n",
      "Epoch 280 of 1000 took 0.707s\n",
      "  training loss (in-iteration):\t\t1.885675\n",
      "  train accuracy:\t\t31.00 %\n",
      "  validation accuracy:\t\t28.70 %\n",
      "Epoch 290 of 1000 took 0.708s\n",
      "  training loss (in-iteration):\t\t1.879803\n",
      "  train accuracy:\t\t31.47 %\n",
      "  validation accuracy:\t\t26.90 %\n",
      "Epoch 300 of 1000 took 0.713s\n",
      "  training loss (in-iteration):\t\t1.885958\n",
      "  train accuracy:\t\t31.07 %\n",
      "  validation accuracy:\t\t28.20 %\n",
      "Epoch 310 of 1000 took 0.705s\n",
      "  training loss (in-iteration):\t\t1.881634\n",
      "  train accuracy:\t\t30.37 %\n",
      "  validation accuracy:\t\t28.40 %\n",
      "Epoch 320 of 1000 took 0.715s\n",
      "  training loss (in-iteration):\t\t1.881757\n",
      "  train accuracy:\t\t31.33 %\n",
      "  validation accuracy:\t\t28.90 %\n",
      "Epoch 330 of 1000 took 0.707s\n",
      "  training loss (in-iteration):\t\t1.894390\n",
      "  train accuracy:\t\t30.33 %\n",
      "  validation accuracy:\t\t29.10 %\n",
      "Epoch 340 of 1000 took 0.707s\n",
      "  training loss (in-iteration):\t\t1.883366\n",
      "  train accuracy:\t\t31.50 %\n",
      "  validation accuracy:\t\t28.40 %\n",
      "Epoch 350 of 1000 took 0.707s\n",
      "  training loss (in-iteration):\t\t1.875253\n",
      "  train accuracy:\t\t31.77 %\n",
      "  validation accuracy:\t\t29.00 %\n",
      "Epoch 360 of 1000 took 0.708s\n",
      "  training loss (in-iteration):\t\t1.870613\n",
      "  train accuracy:\t\t32.03 %\n",
      "  validation accuracy:\t\t29.80 %\n",
      "Epoch 370 of 1000 took 0.727s\n",
      "  training loss (in-iteration):\t\t1.877108\n",
      "  train accuracy:\t\t30.87 %\n",
      "  validation accuracy:\t\t27.50 %\n",
      "Epoch 380 of 1000 took 0.711s\n",
      "  training loss (in-iteration):\t\t1.881399\n",
      "  train accuracy:\t\t31.47 %\n",
      "  validation accuracy:\t\t28.20 %\n",
      "Epoch 390 of 1000 took 0.726s\n",
      "  training loss (in-iteration):\t\t1.868756\n",
      "  train accuracy:\t\t30.63 %\n",
      "  validation accuracy:\t\t31.20 %\n",
      "Epoch 400 of 1000 took 0.711s\n",
      "  training loss (in-iteration):\t\t1.869023\n",
      "  train accuracy:\t\t31.20 %\n",
      "  validation accuracy:\t\t28.40 %\n",
      "Epoch 410 of 1000 took 0.710s\n",
      "  training loss (in-iteration):\t\t1.865790\n",
      "  train accuracy:\t\t32.03 %\n",
      "  validation accuracy:\t\t29.30 %\n",
      "Epoch 420 of 1000 took 0.716s\n",
      "  training loss (in-iteration):\t\t1.874334\n",
      "  train accuracy:\t\t31.00 %\n",
      "  validation accuracy:\t\t28.90 %\n",
      "Epoch 430 of 1000 took 0.725s\n",
      "  training loss (in-iteration):\t\t1.861244\n",
      "  train accuracy:\t\t32.00 %\n",
      "  validation accuracy:\t\t29.60 %\n",
      "Epoch 440 of 1000 took 0.712s\n",
      "  training loss (in-iteration):\t\t1.866364\n",
      "  train accuracy:\t\t31.87 %\n",
      "  validation accuracy:\t\t30.00 %\n",
      "Epoch 450 of 1000 took 0.717s\n",
      "  training loss (in-iteration):\t\t1.866137\n",
      "  train accuracy:\t\t31.17 %\n",
      "  validation accuracy:\t\t29.20 %\n",
      "Epoch 460 of 1000 took 0.718s\n",
      "  training loss (in-iteration):\t\t1.864935\n",
      "  train accuracy:\t\t31.37 %\n",
      "  validation accuracy:\t\t28.60 %\n",
      "Epoch 470 of 1000 took 0.711s\n",
      "  training loss (in-iteration):\t\t1.858671\n",
      "  train accuracy:\t\t32.20 %\n",
      "  validation accuracy:\t\t28.70 %\n",
      "Epoch 480 of 1000 took 0.717s\n",
      "  training loss (in-iteration):\t\t1.868786\n",
      "  train accuracy:\t\t31.30 %\n",
      "  validation accuracy:\t\t28.90 %\n",
      "Epoch 490 of 1000 took 0.717s\n",
      "  training loss (in-iteration):\t\t1.849537\n",
      "  train accuracy:\t\t32.50 %\n",
      "  validation accuracy:\t\t27.30 %\n",
      "Epoch 500 of 1000 took 0.709s\n",
      "  training loss (in-iteration):\t\t1.841343\n",
      "  train accuracy:\t\t32.70 %\n",
      "  validation accuracy:\t\t30.60 %\n",
      "Epoch 510 of 1000 took 0.710s\n",
      "  training loss (in-iteration):\t\t1.856222\n",
      "  train accuracy:\t\t32.33 %\n",
      "  validation accuracy:\t\t29.50 %\n",
      "Epoch 520 of 1000 took 0.709s\n",
      "  training loss (in-iteration):\t\t1.863651\n",
      "  train accuracy:\t\t32.10 %\n",
      "  validation accuracy:\t\t29.50 %\n",
      "Epoch 530 of 1000 took 0.711s\n",
      "  training loss (in-iteration):\t\t1.852615\n",
      "  train accuracy:\t\t32.07 %\n",
      "  validation accuracy:\t\t28.80 %\n",
      "Epoch 540 of 1000 took 0.708s\n",
      "  training loss (in-iteration):\t\t1.843859\n",
      "  train accuracy:\t\t32.13 %\n",
      "  validation accuracy:\t\t30.50 %\n",
      "Epoch 550 of 1000 took 0.710s\n",
      "  training loss (in-iteration):\t\t1.864675\n",
      "  train accuracy:\t\t32.17 %\n",
      "  validation accuracy:\t\t28.30 %\n",
      "Epoch 560 of 1000 took 0.711s\n",
      "  training loss (in-iteration):\t\t1.859919\n",
      "  train accuracy:\t\t32.20 %\n",
      "  validation accuracy:\t\t29.40 %\n",
      "Epoch 570 of 1000 took 0.716s\n",
      "  training loss (in-iteration):\t\t1.837962\n",
      "  train accuracy:\t\t33.17 %\n",
      "  validation accuracy:\t\t30.10 %\n",
      "Epoch 580 of 1000 took 0.710s\n",
      "  training loss (in-iteration):\t\t1.839022\n",
      "  train accuracy:\t\t32.63 %\n",
      "  validation accuracy:\t\t29.10 %\n",
      "Epoch 590 of 1000 took 0.716s\n",
      "  training loss (in-iteration):\t\t1.853177\n",
      "  train accuracy:\t\t32.37 %\n",
      "  validation accuracy:\t\t28.50 %\n",
      "Epoch 600 of 1000 took 0.710s\n",
      "  training loss (in-iteration):\t\t1.852578\n",
      "  train accuracy:\t\t31.83 %\n",
      "  validation accuracy:\t\t29.70 %\n",
      "Epoch 610 of 1000 took 0.711s\n",
      "  training loss (in-iteration):\t\t1.841352\n",
      "  train accuracy:\t\t32.30 %\n",
      "  validation accuracy:\t\t29.70 %\n",
      "Epoch 620 of 1000 took 0.711s\n",
      "  training loss (in-iteration):\t\t1.836735\n",
      "  train accuracy:\t\t33.43 %\n",
      "  validation accuracy:\t\t29.70 %\n",
      "Epoch 630 of 1000 took 0.701s\n",
      "  training loss (in-iteration):\t\t1.844297\n",
      "  train accuracy:\t\t33.57 %\n",
      "  validation accuracy:\t\t28.70 %\n",
      "Epoch 640 of 1000 took 0.710s\n",
      "  training loss (in-iteration):\t\t1.847121\n",
      "  train accuracy:\t\t33.00 %\n",
      "  validation accuracy:\t\t29.10 %\n",
      "Epoch 650 of 1000 took 0.712s\n",
      "  training loss (in-iteration):\t\t1.844534\n",
      "  train accuracy:\t\t32.97 %\n",
      "  validation accuracy:\t\t29.20 %\n",
      "Epoch 660 of 1000 took 0.706s\n",
      "  training loss (in-iteration):\t\t1.837697\n",
      "  train accuracy:\t\t32.57 %\n",
      "  validation accuracy:\t\t28.90 %\n",
      "Epoch 670 of 1000 took 0.707s\n",
      "  training loss (in-iteration):\t\t1.830581\n",
      "  train accuracy:\t\t32.50 %\n",
      "  validation accuracy:\t\t30.10 %\n",
      "Epoch 680 of 1000 took 0.711s\n",
      "  training loss (in-iteration):\t\t1.827484\n",
      "  train accuracy:\t\t33.47 %\n",
      "  validation accuracy:\t\t28.30 %\n",
      "Epoch 690 of 1000 took 0.703s\n",
      "  training loss (in-iteration):\t\t1.850233\n",
      "  train accuracy:\t\t32.63 %\n",
      "  validation accuracy:\t\t29.90 %\n",
      "Epoch 700 of 1000 took 0.710s\n",
      "  training loss (in-iteration):\t\t1.843154\n",
      "  train accuracy:\t\t31.90 %\n",
      "  validation accuracy:\t\t29.70 %\n",
      "Epoch 710 of 1000 took 0.716s\n",
      "  training loss (in-iteration):\t\t1.837295\n",
      "  train accuracy:\t\t33.03 %\n",
      "  validation accuracy:\t\t28.80 %\n",
      "Epoch 720 of 1000 took 0.709s\n",
      "  training loss (in-iteration):\t\t1.827389\n",
      "  train accuracy:\t\t32.93 %\n",
      "  validation accuracy:\t\t29.00 %\n",
      "Epoch 730 of 1000 took 0.633s\n",
      "  training loss (in-iteration):\t\t1.833122\n",
      "  train accuracy:\t\t33.70 %\n",
      "  validation accuracy:\t\t30.20 %\n",
      "Epoch 740 of 1000 took 0.639s\n",
      "  training loss (in-iteration):\t\t1.836057\n",
      "  train accuracy:\t\t32.33 %\n",
      "  validation accuracy:\t\t29.00 %\n",
      "Epoch 750 of 1000 took 0.640s\n",
      "  training loss (in-iteration):\t\t1.838270\n",
      "  train accuracy:\t\t33.93 %\n",
      "  validation accuracy:\t\t29.50 %\n",
      "Epoch 760 of 1000 took 0.639s\n",
      "  training loss (in-iteration):\t\t1.828529\n",
      "  train accuracy:\t\t32.63 %\n",
      "  validation accuracy:\t\t30.80 %\n",
      "Epoch 770 of 1000 took 0.641s\n",
      "  training loss (in-iteration):\t\t1.818244\n",
      "  train accuracy:\t\t33.77 %\n",
      "  validation accuracy:\t\t29.70 %\n",
      "Epoch 780 of 1000 took 0.638s\n",
      "  training loss (in-iteration):\t\t1.825236\n",
      "  train accuracy:\t\t33.30 %\n",
      "  validation accuracy:\t\t29.80 %\n",
      "Epoch 790 of 1000 took 0.634s\n",
      "  training loss (in-iteration):\t\t1.835636\n",
      "  train accuracy:\t\t33.37 %\n",
      "  validation accuracy:\t\t29.00 %\n",
      "Epoch 800 of 1000 took 0.634s\n",
      "  training loss (in-iteration):\t\t1.828647\n",
      "  train accuracy:\t\t33.43 %\n",
      "  validation accuracy:\t\t29.70 %\n",
      "Epoch 810 of 1000 took 0.636s\n",
      "  training loss (in-iteration):\t\t1.833549\n",
      "  train accuracy:\t\t32.77 %\n",
      "  validation accuracy:\t\t31.10 %\n",
      "Epoch 820 of 1000 took 0.644s\n",
      "  training loss (in-iteration):\t\t1.834366\n",
      "  train accuracy:\t\t33.07 %\n",
      "  validation accuracy:\t\t29.70 %\n",
      "Epoch 830 of 1000 took 0.637s\n",
      "  training loss (in-iteration):\t\t1.818615\n",
      "  train accuracy:\t\t33.37 %\n",
      "  validation accuracy:\t\t29.80 %\n",
      "Epoch 840 of 1000 took 0.640s\n",
      "  training loss (in-iteration):\t\t1.820361\n",
      "  train accuracy:\t\t34.30 %\n",
      "  validation accuracy:\t\t28.90 %\n",
      "Epoch 850 of 1000 took 0.639s\n",
      "  training loss (in-iteration):\t\t1.819296\n",
      "  train accuracy:\t\t34.40 %\n",
      "  validation accuracy:\t\t30.10 %\n",
      "Epoch 860 of 1000 took 0.632s\n",
      "  training loss (in-iteration):\t\t1.811566\n",
      "  train accuracy:\t\t35.30 %\n",
      "  validation accuracy:\t\t29.20 %\n",
      "Epoch 870 of 1000 took 0.635s\n",
      "  training loss (in-iteration):\t\t1.803904\n",
      "  train accuracy:\t\t33.93 %\n",
      "  validation accuracy:\t\t29.90 %\n",
      "Epoch 880 of 1000 took 0.639s\n",
      "  training loss (in-iteration):\t\t1.814849\n",
      "  train accuracy:\t\t34.33 %\n",
      "  validation accuracy:\t\t29.90 %\n",
      "Epoch 890 of 1000 took 0.634s\n",
      "  training loss (in-iteration):\t\t1.817882\n",
      "  train accuracy:\t\t33.77 %\n",
      "  validation accuracy:\t\t29.60 %\n",
      "Epoch 900 of 1000 took 0.638s\n",
      "  training loss (in-iteration):\t\t1.804146\n",
      "  train accuracy:\t\t34.20 %\n",
      "  validation accuracy:\t\t31.00 %\n",
      "Epoch 910 of 1000 took 0.639s\n",
      "  training loss (in-iteration):\t\t1.818497\n",
      "  train accuracy:\t\t33.37 %\n",
      "  validation accuracy:\t\t29.30 %\n",
      "Epoch 920 of 1000 took 0.640s\n",
      "  training loss (in-iteration):\t\t1.820719\n",
      "  train accuracy:\t\t34.17 %\n",
      "  validation accuracy:\t\t28.60 %\n",
      "Epoch 930 of 1000 took 0.639s\n",
      "  training loss (in-iteration):\t\t1.803785\n",
      "  train accuracy:\t\t35.27 %\n",
      "  validation accuracy:\t\t30.00 %\n",
      "Epoch 940 of 1000 took 0.640s\n",
      "  training loss (in-iteration):\t\t1.801108\n",
      "  train accuracy:\t\t32.87 %\n",
      "  validation accuracy:\t\t29.50 %\n",
      "Epoch 950 of 1000 took 0.638s\n",
      "  training loss (in-iteration):\t\t1.807477\n",
      "  train accuracy:\t\t34.20 %\n",
      "  validation accuracy:\t\t29.50 %\n",
      "Epoch 960 of 1000 took 0.640s\n",
      "  training loss (in-iteration):\t\t1.808744\n",
      "  train accuracy:\t\t33.87 %\n",
      "  validation accuracy:\t\t30.20 %\n",
      "Epoch 970 of 1000 took 0.639s\n",
      "  training loss (in-iteration):\t\t1.815777\n",
      "  train accuracy:\t\t33.83 %\n",
      "  validation accuracy:\t\t31.20 %\n",
      "Epoch 980 of 1000 took 0.639s\n",
      "  training loss (in-iteration):\t\t1.815418\n",
      "  train accuracy:\t\t33.30 %\n",
      "  validation accuracy:\t\t29.30 %\n",
      "Epoch 990 of 1000 took 0.639s\n",
      "  training loss (in-iteration):\t\t1.806092\n",
      "  train accuracy:\t\t34.00 %\n",
      "  validation accuracy:\t\t29.60 %\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAEaCAYAAADpMdsXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCLEZJQ9CCCE2+3+gTP+HqFfv7gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9540b57ed0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset = train_test_split(X_train.reshape(-1, 3, 32, 32).astype(np.float32), y_train.astype(np.int32))\n",
    "learn(dataset, num_epochs=1000, batchsize=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 of 1000 took 0.688s\n",
      "  training loss (in-iteration):\t\t1.816010\n",
      "  train accuracy:\t\t34.27 %\n",
      "  validation accuracy:\t\t29.80 %\n",
      "Epoch 10 of 1000 took 0.667s\n",
      "  training loss (in-iteration):\t\t1.805129\n",
      "  train accuracy:\t\t34.57 %\n",
      "  validation accuracy:\t\t30.50 %\n",
      "Epoch 20 of 1000 took 0.668s\n",
      "  training loss (in-iteration):\t\t1.804068\n",
      "  train accuracy:\t\t34.33 %\n",
      "  validation accuracy:\t\t31.00 %\n",
      "Epoch 30 of 1000 took 0.671s\n",
      "  training loss (in-iteration):\t\t1.806924\n",
      "  train accuracy:\t\t33.80 %\n",
      "  validation accuracy:\t\t30.10 %\n",
      "Epoch 40 of 1000 took 0.630s\n",
      "  training loss (in-iteration):\t\t1.810378\n",
      "  train accuracy:\t\t34.93 %\n",
      "  validation accuracy:\t\t28.60 %\n",
      "Epoch 50 of 1000 took 0.608s\n",
      "  training loss (in-iteration):\t\t1.810237\n",
      "  train accuracy:\t\t34.70 %\n",
      "  validation accuracy:\t\t29.10 %\n",
      "Epoch 60 of 1000 took 0.610s\n",
      "  training loss (in-iteration):\t\t1.789263\n",
      "  train accuracy:\t\t34.70 %\n",
      "  validation accuracy:\t\t30.50 %\n",
      "Epoch 70 of 1000 took 0.614s\n",
      "  training loss (in-iteration):\t\t1.794184\n",
      "  train accuracy:\t\t35.30 %\n",
      "  validation accuracy:\t\t30.60 %\n",
      "Epoch 80 of 1000 took 0.613s\n",
      "  training loss (in-iteration):\t\t1.795624\n",
      "  train accuracy:\t\t34.27 %\n",
      "  validation accuracy:\t\t29.80 %\n",
      "Epoch 90 of 1000 took 0.620s\n",
      "  training loss (in-iteration):\t\t1.791455\n",
      "  train accuracy:\t\t35.13 %\n",
      "  validation accuracy:\t\t30.00 %\n",
      "Epoch 100 of 1000 took 0.619s\n",
      "  training loss (in-iteration):\t\t1.792332\n",
      "  train accuracy:\t\t34.97 %\n",
      "  validation accuracy:\t\t29.00 %\n",
      "Epoch 110 of 1000 took 0.621s\n",
      "  training loss (in-iteration):\t\t1.804443\n",
      "  train accuracy:\t\t35.20 %\n",
      "  validation accuracy:\t\t28.90 %\n",
      "Epoch 120 of 1000 took 0.622s\n",
      "  training loss (in-iteration):\t\t1.792666\n",
      "  train accuracy:\t\t35.23 %\n",
      "  validation accuracy:\t\t31.80 %\n",
      "Epoch 130 of 1000 took 0.677s\n",
      "  training loss (in-iteration):\t\t1.798080\n",
      "  train accuracy:\t\t35.07 %\n",
      "  validation accuracy:\t\t29.80 %\n",
      "Epoch 140 of 1000 took 0.654s\n",
      "  training loss (in-iteration):\t\t1.805107\n",
      "  train accuracy:\t\t34.93 %\n",
      "  validation accuracy:\t\t28.90 %\n",
      "Epoch 150 of 1000 took 0.634s\n",
      "  training loss (in-iteration):\t\t1.791469\n",
      "  train accuracy:\t\t35.20 %\n",
      "  validation accuracy:\t\t30.20 %\n",
      "Epoch 160 of 1000 took 0.637s\n",
      "  training loss (in-iteration):\t\t1.810587\n",
      "  train accuracy:\t\t34.60 %\n",
      "  validation accuracy:\t\t29.90 %\n",
      "Epoch 170 of 1000 took 0.643s\n",
      "  training loss (in-iteration):\t\t1.806038\n",
      "  train accuracy:\t\t34.07 %\n",
      "  validation accuracy:\t\t30.60 %\n",
      "Epoch 180 of 1000 took 0.643s\n",
      "  training loss (in-iteration):\t\t1.802350\n",
      "  train accuracy:\t\t34.33 %\n",
      "  validation accuracy:\t\t30.80 %\n",
      "Epoch 190 of 1000 took 0.645s\n",
      "  training loss (in-iteration):\t\t1.792658\n",
      "  train accuracy:\t\t35.37 %\n",
      "  validation accuracy:\t\t30.80 %\n",
      "Epoch 200 of 1000 took 0.646s\n",
      "  training loss (in-iteration):\t\t1.784288\n",
      "  train accuracy:\t\t34.97 %\n",
      "  validation accuracy:\t\t29.90 %\n",
      "Epoch 210 of 1000 took 0.645s\n",
      "  training loss (in-iteration):\t\t1.802865\n",
      "  train accuracy:\t\t34.47 %\n",
      "  validation accuracy:\t\t30.10 %\n",
      "Epoch 220 of 1000 took 0.645s\n",
      "  training loss (in-iteration):\t\t1.803144\n",
      "  train accuracy:\t\t34.53 %\n",
      "  validation accuracy:\t\t30.50 %\n",
      "Epoch 230 of 1000 took 0.646s\n",
      "  training loss (in-iteration):\t\t1.788437\n",
      "  train accuracy:\t\t35.73 %\n",
      "  validation accuracy:\t\t30.00 %\n",
      "Epoch 240 of 1000 took 0.646s\n",
      "  training loss (in-iteration):\t\t1.793033\n",
      "  train accuracy:\t\t35.03 %\n",
      "  validation accuracy:\t\t30.40 %\n",
      "Epoch 250 of 1000 took 0.646s\n",
      "  training loss (in-iteration):\t\t1.792962\n",
      "  train accuracy:\t\t34.33 %\n",
      "  validation accuracy:\t\t30.10 %\n",
      "Epoch 260 of 1000 took 0.646s\n",
      "  training loss (in-iteration):\t\t1.787971\n",
      "  train accuracy:\t\t35.30 %\n",
      "  validation accuracy:\t\t29.80 %\n",
      "Epoch 270 of 1000 took 0.645s\n",
      "  training loss (in-iteration):\t\t1.784392\n",
      "  train accuracy:\t\t34.60 %\n",
      "  validation accuracy:\t\t28.90 %\n",
      "Epoch 280 of 1000 took 0.645s\n",
      "  training loss (in-iteration):\t\t1.788658\n",
      "  train accuracy:\t\t35.57 %\n",
      "  validation accuracy:\t\t28.30 %\n",
      "Epoch 290 of 1000 took 0.645s\n",
      "  training loss (in-iteration):\t\t1.784857\n",
      "  train accuracy:\t\t35.20 %\n",
      "  validation accuracy:\t\t29.50 %\n",
      "Epoch 300 of 1000 took 0.645s\n",
      "  training loss (in-iteration):\t\t1.782547\n",
      "  train accuracy:\t\t35.33 %\n",
      "  validation accuracy:\t\t29.40 %\n",
      "Epoch 310 of 1000 took 0.653s\n",
      "  training loss (in-iteration):\t\t1.776048\n",
      "  train accuracy:\t\t35.70 %\n",
      "  validation accuracy:\t\t29.50 %\n",
      "Epoch 320 of 1000 took 0.639s\n",
      "  training loss (in-iteration):\t\t1.784310\n",
      "  train accuracy:\t\t35.83 %\n",
      "  validation accuracy:\t\t30.60 %\n",
      "Epoch 330 of 1000 took 0.645s\n",
      "  training loss (in-iteration):\t\t1.793892\n",
      "  train accuracy:\t\t35.03 %\n",
      "  validation accuracy:\t\t29.00 %\n",
      "Epoch 340 of 1000 took 0.653s\n",
      "  training loss (in-iteration):\t\t1.797482\n",
      "  train accuracy:\t\t34.30 %\n",
      "  validation accuracy:\t\t29.80 %\n",
      "Epoch 350 of 1000 took 0.639s\n",
      "  training loss (in-iteration):\t\t1.783258\n",
      "  train accuracy:\t\t35.40 %\n",
      "  validation accuracy:\t\t29.10 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-2f3860cc96f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-a0c594748dc3>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(dataset, num_epochs, batchsize)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterate_minibatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mtrain_err_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0mtrain_err\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_err_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mtrain_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_acc_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mborisya/opt/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learn(dataset, num_epochs=1000, batchsize=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learn(dataset, num_epochs=1000, batchsize=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f9517d7bc10>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f953fa99f10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "im = X_train.reshape(-1, 32, 32, 3)[np.random.choice(X_train.shape[0])]\n",
    "plt.imshow(im * 255 + 1 *  np.array([36, 39, 40]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-36"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(im * 255)[:, :, 0].astype(np.int).ravel().min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 3, 32)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im.transpose(1, 2, 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (32,3,32) (3,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-147-eb28f6672364>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (32,3,32) (3,) "
     ]
    }
   ],
   "source": [
    "plt.imshow(im.transpose(1, 2, 0) + np.array([0.25, 0.2, 0.2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 118.49177095,  127.0937194 ,  133.98695975],\n",
       "        [ 116.6498371 ,  136.0862944 ,  131.1889289 ],\n",
       "        [  96.2504797 ,  125.57739505,  131.21186105],\n",
       "        ..., \n",
       "        [ 103.03549802,  125.24395705,  129.65861015],\n",
       "        [ 106.68621445,  118.17013643,  123.27558479],\n",
       "        [ 104.00431886,  116.61156119,  123.19729061]],\n",
       "\n",
       "       [[ 101.87356375,  116.11873538,  124.00105801],\n",
       "        [ 103.40014037,  114.1915354 ,  121.76781085],\n",
       "        [ 103.06466008,  111.5255818 ,  113.4631751 ],\n",
       "        ..., \n",
       "        [ 102.19314479,  115.631717  ,  112.69138445],\n",
       "        [  99.87034495,  127.72411255,  128.55336365],\n",
       "        [ 105.34682236,  116.64011394,  123.24799022]],\n",
       "\n",
       "       [[ 101.61038003,  115.97705738,  126.19946171],\n",
       "        [ 105.78716197,  115.26210011,  122.19450056],\n",
       "        [ 101.2847968 ,  105.2105323 ,  119.52563945],\n",
       "        ..., \n",
       "        [ 102.38828507,  115.16269576,  121.63420156],\n",
       "        [  98.67933175,  106.4779486 ,  124.78811423],\n",
       "        [ 117.97615585,  123.55160365,  127.2255353 ]],\n",
       "\n",
       "       ..., \n",
       "       [[ 105.10307143,  113.99801335,  124.09898948],\n",
       "        [ 112.60802845,  112.6383508 ,  113.6303786 ],\n",
       "        [ 101.79700969,  111.47911825,  126.9876356 ],\n",
       "        ..., \n",
       "        [ 101.89774973,  108.5917099 ,  119.3024048 ],\n",
       "        [  97.53068425,  104.9291296 ,  120.2274683 ],\n",
       "        [ 107.0470318 ,  123.00803035,  126.194695  ]],\n",
       "\n",
       "       [[ 102.35267075,  116.61086776,  124.38573214],\n",
       "        [ 107.7301819 ,  124.8346183 ,  139.1959646 ],\n",
       "        [ 111.40415305,  102.09228775,  106.93563155],\n",
       "        ..., \n",
       "        [ 103.81910897,  118.17874217,  124.34823872],\n",
       "        [ 103.98272643,  115.16711976,  122.28980681],\n",
       "        [ 102.37265791,  115.52902545,  123.92907898]],\n",
       "\n",
       "       [[ 101.99456507,  114.52360941,  122.10792347],\n",
       "        [ 103.58881258,  116.39610628,  122.81471972],\n",
       "        [ 104.50116866,  117.79883221,  125.21589611],\n",
       "        ..., \n",
       "        [ 107.5332505 ,  120.59843335,  127.62919775],\n",
       "        [ 108.695413  ,  122.438113  ,  127.7925992 ],\n",
       "        [ 106.51753705,  118.45673349,  127.52728955]]])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(im * 255 + np.array([103.939, 116.779, 123.68])) / "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_generator(imshape):\n",
    "    net = InputLayer(imshape)\n",
    "    net = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('ds.pkl', 'wb') as f:\n",
    "    pickle.dump(X_train, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('ds_labels.pkl', 'wb') as f:\n",
    "    pickle.dump(y_train, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
